

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>TensorFlow 模型建立與訓練 &mdash; 简单粗暴 TensorFlow 2 0.4 beta 文档</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/js/tw_cn.js"></script>
        <script src="../../_static/js/pangu.min.js"></script>
        <script src="../../_static/js/custom_20200921.js"></script>
        <script src="../../_static/translations.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="TensorFlow常用模組" href="tools.html" />
    <link rel="prev" title="TensorFlow 基礎" href="basic.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> 简单粗暴 TensorFlow 2
          

          
          </a>

          
            
            
              <div class="version">
                0.4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">目录</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/foreword.html">推荐序</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/preface.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/introduction.html">TensorFlow概述</a></li>
</ul>
<p class="caption"><span class="caption-text">基础</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/installation.html">TensorFlow安装与环境配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/basic.html">TensorFlow基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/models.html">TensorFlow 模型建立与训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/tools.html">TensorFlow常用模块</a></li>
</ul>
<p class="caption"><span class="caption-text">部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/export.html">TensorFlow模型导出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/serving.html">TensorFlow Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/lite.html">TensorFlow Lite（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/javascript.html">TensorFlow in JavaScript（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">大规模训练与加速</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/distributed.html">TensorFlow分布式训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/tpu.html">使用TPU训练TensorFlow模型（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">扩展</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/tfhub.html">TensorFlow Hub 模型复用（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/tfds.html">TensorFlow Datasets 数据集载入</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/swift.html">Swift for TensorFlow (S4TF) (Huan）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/quantum.html">TensorFlow Quantum: 混合量子-经典机器学习 *</a></li>
</ul>
<p class="caption"><span class="caption-text">附录</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/rl.html">强化学习简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/docker.html">使用Docker部署TensorFlow环境</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/cloud.html">在云端使用TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/jupyterlab.html">部署自己的交互式Python开发环境JupyterLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/recommended_books.html">参考资料与推荐阅读</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/terms.html">术语中英对照表</a></li>
</ul>
<p class="caption"><span class="caption-text">目錄</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../preface.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">TensorFlow概述</a></li>
</ul>
<p class="caption"><span class="caption-text">基礎</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">TensorFlow 安裝與環境配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic.html">TensorFlow 基礎</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">TensorFlow 模型建立與訓練</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model-layer">模型（Model）與層（Layer）</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mlp">基礎範例：多層感知器（MLP）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tf-keras-datasets">資料獲取及預處理： <code class="docutils literal notranslate"><span class="pre">tf.keras.datasets</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tf-keras-model-tf-keras-layers">模型的建構： <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 和 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tf-keras-losses-tf-keras-optimizer">模型的訓練： <code class="docutils literal notranslate"><span class="pre">tf.keras.losses</span></code> 和 <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizer</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tf-keras-metrics">模型的評估： <code class="docutils literal notranslate"><span class="pre">tf.keras.metrics</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#cnn">卷積神經網路（CNN）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#keras">使用Keras實現卷積神經網路</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id14">使用Keras中預定義的典型卷積神經網路結構</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#rnn">循環神經網路（RNN）</a></li>
<li class="toctree-l2"><a class="reference internal" href="#drl">深度強化學習（DRL）</a></li>
<li class="toctree-l2"><a class="reference internal" href="#keras-pipeline">Keras Pipeline *</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#keras-sequential-functional-api">Keras Sequential/Functional API 模式建立模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="#keras-model-compile-fit-evaluate">使用 Keras Model 的 <code class="docutils literal notranslate"><span class="pre">compile</span></code> 、 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 和 <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> 方法訓練和評估模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id23">自定義層、損失函數和評量指標 *</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#custom-layer">自定義層</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id25">自定義損失函數和評量指標</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">TensorFlow常用模組</a></li>
</ul>
<p class="caption"><span class="caption-text">部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deployment/export.html">TensorFlow模型匯出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/serving.html">TensorFlow Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/lite.html">TensorFlow Lite（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/javascript.html">TensorFlow in JavaScript（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">大規模訓練與加速</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix/distributed.html">TensorFlow分布式訓練</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/tpu.html">使用TPU訓練TensorFlow模型（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">擴展</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix/tfhub.html">TensorFlow Hub 模型複用（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/tfds.html">TensorFlow Datasets 資料集載入</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/swift.html">Swift for TensorFlow (S4TF) (Huan）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/quantum.html">TensorFlow Quantum: 混合量子-經典機器學習 *</a></li>
</ul>
<p class="caption"><span class="caption-text">附錄</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix/rl.html">強化學習簡介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/docker.html">使用Docker部署TensorFlow環境</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/cloud.html">在雲端使用TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/jupyterlab.html">部署自己的互動式 Python 開發環境 JupyterLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/recommended_books.html">參考資料與推薦閱讀</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/terms.html">專有名詞中英對照表</a></li>
</ul>
<p class="caption"><span class="caption-text">Preface</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/introduction.html">TensorFlow Overview</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/installation.html">Installation and Environment Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/basic.html">TensorFlow Basic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/models.html">Model Construction and Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/tools.html">Common Modules in TensorFlow</a></li>
</ul>
<p class="caption"><span class="caption-text">Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/deployment/export.html">TensorFlow Model Export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/deployment/serving.html">TensorFlow Serving</a></li>
</ul>
<p class="caption"><span class="caption-text">Large-scale Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/appendix/distributed.html">Distributed training with TensorFlow</a></li>
</ul>
<p class="caption"><span class="caption-text">Extensions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/appendix/tfds.html">TensorFlow Datasets: Ready-to-use Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/appendix/quantum.html">TensorFlow Quantum: Hybrid Quantum-classical Machine Learning *</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">简单粗暴 TensorFlow 2</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>TensorFlow 模型建立與訓練</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/zh_hant/basic/models.rst.txt" rel="nofollow"> 查看页面源码</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tensorflow">
<h1>TensorFlow 模型建立與訓練<a class="headerlink" href="#tensorflow" title="永久链接至标题">¶</a></h1>
<p id="linear">本章介紹如何使用 TensorFlow 快速建立動態模型。</p>
<ul class="simple">
<li><p>模型的建構： <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 和 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers</span></code></p></li>
<li><p>模型的損失函數： <code class="docutils literal notranslate"><span class="pre">tf.keras.losses</span></code></p></li>
<li><p>模型的優化器： <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizer</span></code></p></li>
<li><p>模型的評估： <code class="docutils literal notranslate"><span class="pre">tf.keras.metrics</span></code></p></li>
</ul>
<div class="admonition- admonition">
<p class="admonition-title">前置知識</p>
<ul class="simple">
<li><p><a class="reference external" href="https://openhome.cc/Gossip/CodeData/PythonTutorial/FunctionModuleClassPackagePy3.html">Python  物件導向程式語言</a> （在 Python 內定義類別的方法、類別的繼承、建構和解構函式，<a class="reference external" href="https://medium.com/&#64;dboyliao/python-%E7%B9%BC%E6%89%BF-543-bc3d8ef51d6ds">使用 super() 函數呼叫父類方法</a> ，<a class="reference external" href="https://kknews.cc/zh-tw/code/z9p8rvg.html">使用__call__() 方法對實例進行呼叫</a> 等）；</p></li>
<li><p>多層感知器(Multilayer Perceptron, MLP)、卷積神經網路、循環神經網路和強化學習(Reinforcement Learning, RL)（每節之前給出參考資料）。</p></li>
<li><p><a class="reference external" href="https://medium.com/citycoddee/python%E9%80%B2%E9%9A%8E%E6%8A%80%E5%B7%A7-3-%E7%A5%9E%E5%A5%87%E5%8F%88%E7%BE%8E%E5%A5%BD%E7%9A%84-decorator-%E5%97%B7%E5%97%9A-6559edc87bc0">Python 的裝飾器</a> （非必須）</p></li>
</ul>
</div>
<div class="section" id="model-layer">
<h2>模型（Model）與層（Layer）<a class="headerlink" href="#model-layer" title="永久链接至标题">¶</a></h2>
<p>在 TensorFlow 中，推薦使用 Keras（ <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> ）建立模型。Keras 是一個廣為流行的高級神經網路 API，簡單、快速而不失靈活性，現已得到 TensorFlow 的官方內建和全面支援。</p>
<p>Keras 有兩個重要的概念： <strong>模型（Model）</strong> 和 <strong>層（Layer）</strong> 。層將各種計算流程和變數進行了封裝（例如基本的全連接層(Fully Connected Layer)，CNN 的卷積層(Convolution Layer)、池化層(Pooling Layer)…等），而模型則將各種層進行組織和連接，並封裝成一個整體，描述了如何將輸入資料通過各種層以及運算而得到輸出。在需要模型呼叫的時候，使用 <code class="docutils literal notranslate"><span class="pre">y_pred</span> <span class="pre">=</span> <span class="pre">model(X)</span></code> 的形式即可。Keras 在 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers</span></code> 下內建了深度學習中大量常用的的預定義層，同時也允許我們自定義層。</p>
<p>Keras 模型以類別的形式呈現，我們可以通過繼承 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 這個 Python 類別來定義自己的模型。在繼承類別中，我們需要重寫 <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> （建構函數，初始化）和 <code class="docutils literal notranslate"><span class="pre">call(input)</span></code> （模型呼叫）兩個方法，同時也可以根據需要增加自定義的方法。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>     <span class="c1"># Python 2 下使用 super(MyModel, self).__init__()</span>
        <span class="c1"># 此處添加初始化程式碼（包含 call 方法中會用到的層），例如</span>
        <span class="c1"># layer1 = tf.keras.layers.BuiltInLayer(...)</span>
        <span class="c1"># layer2 = MyCustomLayer(...)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># 此處添加模型呼叫的程式碼（處理輸入並返回輸出），例如</span>
        <span class="c1"># x = layer1(input)</span>
        <span class="c1"># output = layer2(x)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="c1"># 還可以添加自定義的方法</span>
</pre></div>
</div>
<div class="figure align-center" id="id26">
<a class="reference internal image-reference" href="../../_images/model_cn.png"><img alt="../../_images/model_cn.png" src="../../_images/model_cn.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-text">Keras 模型類別定義示意圖</span><a class="headerlink" href="#id26" title="永久链接至图片">¶</a></p>
</div>
<p>繼承 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 後，我們同時可以使用父類的若干方法和屬性，例如在實例化類 <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Model()</span></code> 後，可以通過 <code class="docutils literal notranslate"><span class="pre">model.variables</span></code> 這一屬性直接獲得模型中的所有變數，免去我們一個個顯示指定變數的麻煩。</p>
<p>上一章中簡單的線性模型 <code class="docutils literal notranslate"><span class="pre">y_pred</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">*</span> <span class="pre">X</span> <span class="pre">+</span> <span class="pre">b</span></code> ，我們可以通過Model類別的方式編寫如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">10.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">20.0</span><span class="p">]])</span>


<span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">(),</span>
            <span class="n">bias_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>


<span class="c1"># 以下程式碼結構與前一節類似</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>      <span class="c1"># 呼叫模型 y_pred = model(X) 而不是顯式寫出 y_pred = a * X + b</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>    <span class="c1"># 使用 model.variables 這一屬性直接獲得模型中的所有變數</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads_and_vars</span><span class="o">=</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
</pre></div>
</div>
<p>這裡，我們沒有顯式宣告 <code class="docutils literal notranslate"><span class="pre">a</span></code> 和 <code class="docutils literal notranslate"><span class="pre">b</span></code> 兩個變數並寫出 <code class="docutils literal notranslate"><span class="pre">y_pred</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">*</span> <span class="pre">X</span> <span class="pre">+</span> <span class="pre">b</span></code> 這一線性變換，而是建立了一個繼承了 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 的模型類 <code class="docutils literal notranslate"><span class="pre">Linear</span></code> 。這個類別在初始化部分實例了一個 <strong>全連接層</strong> （ <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Dense</span></code> ），並在 call 方法中對這個層進行呼叫，實現了線性變換的計算。如果需要顯式宣告自己的變數並使用變數進行自定義運算，或者希望了解 Keras 層的內部原理，請參考 <a class="reference internal" href="#custom-layer"><span class="std std-ref">自定義層</span></a>。</p>
<div class="admonition-keras admonition">
<p class="admonition-title">Keras 的全連接層：線性變換 + 激活函數</p>
<p><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense">全連接層</a> （Fully-connected Layer，<code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Dense</span></code> ）是 Keras 中最基礎和常用的層之一，對輸入矩陣 <img class="math" src="../../_images/math/b86d9659948ffa756133d580cfc95f923705c2b5.png" alt="A"/> 進行 <img class="math" src="../../_images/math/92bb903eabd575a64bd61bd559678418cda7afb2.png" alt="f(AW + b)"/> 的線性變換 + 激活函數操作。如果不指定激活函數，即是純粹的線性變換 <img class="math" src="../../_images/math/62bbbb16ac53d3269d7aa9fc5a15a986f8f0f08c.png" alt="AW + b"/>。具體而言，給定輸入張量 <code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">=</span> <span class="pre">[batch_size,</span> <span class="pre">input_dim]</span></code> ，該層對輸入張量首先進行 <code class="docutils literal notranslate"><span class="pre">tf.matmul(input,</span> <span class="pre">kernel)</span> <span class="pre">+</span> <span class="pre">bias</span></code> 的線性變換（ <code class="docutils literal notranslate"><span class="pre">kernel</span></code> 和 <code class="docutils literal notranslate"><span class="pre">bias</span></code> 是層中可訓練的變量），然後對線性變換後張量的每個元素通過激活函數 <code class="docutils literal notranslate"><span class="pre">activation</span></code> ，從而輸出形狀為 <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">units]</span></code> 的二維張量。</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/dense.png"><img alt="../../_images/dense.png" src="../../_images/dense.png" style="width: 60%;" /></a>
</div>
<p>其包含的主要參數如下：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">units</span></code> ：輸出張量的維度；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">activation</span></code> ：激活函數，對應於 <img class="math" src="../../_images/math/92bb903eabd575a64bd61bd559678418cda7afb2.png" alt="f(AW + b)"/> 中的 <img class="math" src="../../_images/math/95dc71c95728695dd2fada939d146ae97def5061.png" alt="f"/> ，預設為無激活函數（ <code class="docutils literal notranslate"><span class="pre">a(x)</span> <span class="pre">=</span> <span class="pre">x</span></code> ）。常用的激活函數包括 <code class="docutils literal notranslate"><span class="pre">tf.nn.relu</span></code> 、 <code class="docutils literal notranslate"><span class="pre">tf.nn.tanh</span></code> 和 <code class="docutils literal notranslate"><span class="pre">tf.nn.sigmoid</span></code> ；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_bias</span></code> ：是否加入偏移量 <code class="docutils literal notranslate"><span class="pre">bias</span></code> ，即 <img class="math" src="../../_images/math/92bb903eabd575a64bd61bd559678418cda7afb2.png" alt="f(AW + b)"/> 中的 <img class="math" src="../../_images/math/3de2ed824326dc0eb383a42dd6ec44d3b401047b.png" alt="b"/>。預設為 <code class="docutils literal notranslate"><span class="pre">True</span></code> ；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kernel_initializer</span></code> 、 <code class="docutils literal notranslate"><span class="pre">bias_initializer</span></code> ：權重矩陣 <code class="docutils literal notranslate"><span class="pre">kernel</span></code> 和偏移量 <code class="docutils literal notranslate"><span class="pre">bias</span></code> 兩個變數的初始化器。預設為 <code class="docutils literal notranslate"><span class="pre">tf.glorot_uniform_initializer</span></code> <a class="footnote-reference brackets" href="#glorot" id="id3">1</a> 。設置為 <code class="docutils literal notranslate"><span class="pre">tf.zeros_initializer</span></code> 表示將兩個變量均初始化為全 0；</p></li>
</ul>
<p>該層包含權重矩陣 <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">=</span> <span class="pre">[input_dim,</span> <span class="pre">units]</span></code> 和偏移量 <code class="docutils literal notranslate"><span class="pre">bias</span> <span class="pre">=</span> <span class="pre">[units]</span></code> <a class="footnote-reference brackets" href="#broadcast" id="id4">2</a> 兩個可訓練變數，對應於 <img class="math" src="../../_images/math/92bb903eabd575a64bd61bd559678418cda7afb2.png" alt="f(AW + b)"/> 中的 <img class="math" src="../../_images/math/eb33eca75f5d77363c991bf041953da54a858bfb.png" alt="W"/> 和 <img class="math" src="../../_images/math/3de2ed824326dc0eb383a42dd6ec44d3b401047b.png" alt="b"/>。</p>
<p>這裡著重從數學矩陣運算和線性變換的角度描述了全連接層。基於神經元建模的描述可參考 <a class="reference internal" href="#neuron"><span class="std std-ref">後文介紹</span></a> 。</p>
<dl class="footnote brackets">
<dt class="label" id="glorot"><span class="brackets"><a class="fn-backref" href="#id3">1</a></span></dt>
<dd><p>Keras 中的很多層都預設使用 <code class="docutils literal notranslate"><span class="pre">tf.glorot_uniform_initializer</span></code> 初始化變數，關於該初始化器可參考 <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform">https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform</a>。</p>
</dd>
<dt class="label" id="broadcast"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p>你可能會注意到， <code class="docutils literal notranslate"><span class="pre">tf.matmul(input,</span> <span class="pre">kernel)</span></code> 的結果是一個形狀為 <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">units]</span></code> 的二維矩陣，這個二維矩陣要如何與形狀為 <code class="docutils literal notranslate"><span class="pre">[units]</span></code> 的一維偏移量 <code class="docutils literal notranslate"><span class="pre">bias</span></code> 相加呢？事實上，這裡是 TensorFlow 的 Broadcasting 機制在起作用，該加法運算相當於將二維矩陣的每一行加上了 <code class="docutils literal notranslate"><span class="pre">Bias</span></code> 。Broadcasting 機制的具體介紹可見 <a class="reference external" href="https://www.tensorflow.org/xla/broadcasting">https://www.tensorflow.org/xla/broadcasting</a> 。</p>
</dd>
</dl>
</div>
<div class="admonition-call-call admonition">
<p class="admonition-title">為什麼模型類是重新呼叫 <code class="docutils literal notranslate"><span class="pre">call()</span></code> 方法而不是  <code class="docutils literal notranslate"><span class="pre">__call__()</span></code> 方法？</p>
<p>在 Python 中，對類別的實例 <code class="docutils literal notranslate"><span class="pre">myClass</span></code> 進行形如 <code class="docutils literal notranslate"><span class="pre">myClass()</span></code> 的呼叫等價於 <code class="docutils literal notranslate"><span class="pre">myClass.__call__()</span></code> （具體請見本章初 “前置知識” 的 <code class="docutils literal notranslate"><span class="pre">__call__()</span></code> 部分）。那麼看起來，為了使用 <code class="docutils literal notranslate"><span class="pre">y_pred</span> <span class="pre">=</span> <span class="pre">model(X)</span></code> 的形式呼叫模型類別，應該重寫 <code class="docutils literal notranslate"><span class="pre">__call__()</span></code> 方法才對呀？原因是 Keras 在模型呼叫的前後還需要有一些自己的內部操作，所以暴露出一個專門用於重新呼叫的 <code class="docutils literal notranslate"><span class="pre">call()</span></code> 方法。 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 這一父類已經包含 <code class="docutils literal notranslate"><span class="pre">__call__()</span></code> 的定義。 <code class="docutils literal notranslate"><span class="pre">__call__()</span></code> 中主要呼叫了 <code class="docutils literal notranslate"><span class="pre">call()</span></code> 方法，同時還需要在進行一些 keras 的內部操作。這裡，我們通過繼承 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 並重新呼叫 <code class="docutils literal notranslate"><span class="pre">call()</span></code> 方法，即可在保持 keras 結構的同時加入模型呼叫的程式碼。</p>
</div>
</div>
<div class="section" id="mlp">
<span id="id5"></span><h2>基礎範例：多層感知器（MLP）<a class="headerlink" href="#mlp" title="永久链接至标题">¶</a></h2>
<p>我們從編寫一個最簡單的 <a class="reference external" href="https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8">多層感知器</a> （Multilayer Perceptron, MLP），或者說 “多層全連接神經網路” 開始，介紹 TensorFlow 的模型編寫方式。在這一部分，我們依次進行以下步驟：</p>
<ul class="simple">
<li><p>使用 <code class="docutils literal notranslate"><span class="pre">tf.keras.datasets</span></code> 獲得資料集並預處理</p></li>
<li><p>使用 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 和 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers</span></code> 建構模型</p></li>
<li><p>建構模型訓練流程，使用 <code class="docutils literal notranslate"><span class="pre">tf.keras.losses</span></code> 計算損失函數，並使用 <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizer</span></code> 優化模型</p></li>
<li><p>構建模型評估流程，使用 <code class="docutils literal notranslate"><span class="pre">tf.keras.metrics</span></code> 計算評量指標</p></li>
</ul>
<div class="admonition- admonition">
<p class="admonition-title">基礎知識和原理</p>
<ul class="simple">
<li><p>UFLDL 教程 <a class="reference external" href="http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/">Multi-Layer Neural Network</a> 一節；</p></li>
<li><p>史丹佛大學課程 <a class="reference external" href="https://cs231n.github.io/">CS231n: Convolutional Neural Networks for Visual Recognition</a> 中的 “Neural Networks Part 1 ~ 3” 部分。</p></li>
</ul>
</div>
<p>這裡，我們使用多層感知器完成 MNIST 手寫體數字圖片資料集 <a class="reference internal" href="#lecun1998" id="id7"><span>[LeCun1998]</span></a> 的分類任務。</p>
<div class="figure align-center" id="id27">
<img alt="../../_images/mnist_0-9.png" src="../../_images/mnist_0-9.png" />
<p class="caption"><span class="caption-text">MNIST 手寫體數字圖片範例</span><a class="headerlink" href="#id27" title="永久链接至图片">¶</a></p>
</div>
<div class="section" id="tf-keras-datasets">
<h3>資料獲取及預處理： <code class="docutils literal notranslate"><span class="pre">tf.keras.datasets</span></code><a class="headerlink" href="#tf-keras-datasets" title="永久链接至标题">¶</a></h3>
<p>先進行預備工作，實現一個簡單的 <code class="docutils literal notranslate"><span class="pre">MNISTLoader</span></code> 類來讀取 MNIST 資料集資料。這裡使用了 <code class="docutils literal notranslate"><span class="pre">tf.keras.datasets</span></code> 快速載入 MNIST 資料集。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MNISTLoader</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">mnist</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_label</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">test_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_label</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
        <span class="c1"># MNIST中的圖片預設為uint8（0-255的數字）。以下程式碼將其正規化到0-1之間的浮點數，並在最後增加一維作為顏色通道</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># [60000, 28, 28, 1]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">test_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>        <span class="c1"># [10000, 28, 28, 1]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_label</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>    <span class="c1"># [60000]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_label</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>      <span class="c1"># [10000]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_train_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_test_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="c1"># 從資料集中隨機取出batch_size個元素並返回</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_label</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p><code class="docutils literal notranslate"><span class="pre">mnist</span> <span class="pre">=</span> <span class="pre">tf.keras.datasets.mnist</span></code> 將從網路上自動下載 MNIST 資料集並加載。如果運行時出現網路連接錯誤，可以從 <a class="reference external" href="https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz">https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz</a> 或 <a class="reference external" href="https://s3.amazonaws.com/img-datasets/mnist.npz">https://s3.amazonaws.com/img-datasets/mnist.npz</a> 下載 MNIST 資料集資料。這裡使用了 <code class="docutils literal notranslate"><span class="pre">mnist.npz</span></code> 文件，並放置於使用者目錄的 <code class="docutils literal notranslate"><span class="pre">.keras/dataset</span></code> 目錄下（Windows 下使用者目錄為 <code class="docutils literal notranslate"><span class="pre">C:\Users\使用者名</span></code> ，Linux 下使用者目錄為 <code class="docutils literal notranslate"><span class="pre">/home/使用者名</span></code> ）。</p>
</div>
<div class="admonition-tensorflow admonition">
<p class="admonition-title">TensorFlow 的圖像資料表示</p>
<p>在 TensorFlow 中，圖片資料集的一種典型表示是 <code class="docutils literal notranslate"><span class="pre">[圖片數目，長，寬，色彩通道數]</span></code> 的四維張量。在上面的 <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 類中， <code class="docutils literal notranslate"><span class="pre">self.train_data</span></code> 和 <code class="docutils literal notranslate"><span class="pre">self.test_data</span></code> 分別載入了 60,000 和 10,000 張大小為 <code class="docutils literal notranslate"><span class="pre">28*28</span></code> 的手寫體數字圖片。由於這裡讀入的是灰階圖片，色彩通道數為 1（彩色 RGB 圖像色彩通道數為 3），所以我們使用 <code class="docutils literal notranslate"><span class="pre">np.expand_dims()</span></code> 函數為圖片資料手動在最後添加一維通道。</p>
</div>
</div>
<div class="section" id="tf-keras-model-tf-keras-layers">
<span id="mlp-model"></span><h3>模型的建構： <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 和 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers</span></code><a class="headerlink" href="#tf-keras-model-tf-keras-layers" title="永久链接至标题">¶</a></h3>
<p>多層感知器的模型類別實現與上面的線性模型類似，使用 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 和 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers</span></code> 建構，所不同的地方在於層數增加了（顧名思義，“多層” 感知器），以及引入了非線性激活函數（這裡使用了 <a class="reference external" href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0">ReLU 函數</a> ， 即下方的 <code class="docutils literal notranslate"><span class="pre">activation=tf.nn.relu</span></code> ）。該模型輸入一個向量（比如這裡是拉直的 <code class="docutils literal notranslate"><span class="pre">1×784</span></code> 手寫體數字圖片），輸出 10 維的向量，分別代表這張圖片屬於 0 到 9 的機率。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>    <span class="c1"># Flatten層將除第一維（batch_size）以外的維度展平</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>         <span class="c1"># [batch_size, 28, 28, 1]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>    <span class="c1"># [batch_size, 784]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>          <span class="c1"># [batch_size, 100]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>          <span class="c1"># [batch_size, 10]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<div class="admonition-softmax admonition">
<p class="admonition-title">softmax 函數</p>
<p>這裡，因為我們希望輸出 “輸入圖片分別屬於 0 到 1 的機率”，也就是一個 10 維的離散機率分佈，所以我們希望這個 10 維向量至少滿足兩個條件：</p>
<ul class="simple">
<li><p>該向量中的每個元素均在 <img class="math" src="../../_images/math/1a9360a164e54bf10a49ee770650287ce1e13124.png" alt="[0, 1]"/> 之間；</p></li>
<li><p>該向量的所有元素之和為 1。</p></li>
</ul>
<p>為了使得模型的輸出能始終滿足這兩個條件，我們使用 <a class="reference external" href="https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0">Softmax 函數</a> （正規化指數函數， <code class="docutils literal notranslate"><span class="pre">tf.nn.softmax</span></code> ）對模型的原始輸出進行正規化。其形式為 <img class="math" src="../../_images/math/775c23d38cbe9592ed4cf18946b16b1598501447.png" alt="\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}"/> 。不僅如此，softmax 函數能夠凸顯原始向量中最大的值，並抑制低於最大值的其他分量，這也是該函數被稱作 softmax 函數的原因（即平滑化的 argmax 函數）。</p>
</div>
<div class="figure align-center" id="id28">
<a class="reference internal image-reference" href="../../_images/mlp.png"><img alt="../../_images/mlp.png" src="../../_images/mlp.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-text">MLP 模型示意圖</span><a class="headerlink" href="#id28" title="永久链接至图片">¶</a></p>
</div>
</div>
<div class="section" id="tf-keras-losses-tf-keras-optimizer">
<h3>模型的訓練： <code class="docutils literal notranslate"><span class="pre">tf.keras.losses</span></code> 和 <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizer</span></code><a class="headerlink" href="#tf-keras-losses-tf-keras-optimizer" title="永久链接至标题">¶</a></h3>
<p>定義一些模型超參數(Hyperparameters)：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
</pre></div>
</div>
<p>實例化模型和資料讀取類，並實例化一個 <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizer</span></code> 的優化器（這裡使用常用的 Adam 優化器）：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">()</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">MNISTLoader</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
</div>
<p>然後疊代進行以下步驟：</p>
<ul class="simple">
<li><p>從 DataLoader 中隨機取一批訓練資料；</p></li>
<li><p>將這批資料送入模型，計算出模型的預測值；</p></li>
<li><p>將模型預測值與真實值進行比較，計算損失函數（loss）。這里使用 <code class="docutils literal notranslate"><span class="pre">tf.keras.losses</span></code> 中的交叉熵函數作為損失函數；</p></li>
<li><p>計算損失函數關於模型變數的導數；</p></li>
<li><p>將求出的導數值傳入優化器，使用優化器的 <code class="docutils literal notranslate"><span class="pre">apply_gradients</span></code> 方法更新模型參數以最小化損失函數（優化器的詳細使用方法見 <a class="reference internal" href="basic.html#optimizer"><span class="std std-ref">前章</span></a>  ）。</p></li>
</ul>
<p>具體代碼實現如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">num_batches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">data_loader</span><span class="o">.</span><span class="n">num_train_data</span> <span class="o">//</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_epochs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">batch_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data_loader</span><span class="o">.</span><span class="n">get_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;batch </span><span class="si">%d</span><span class="s2">: loss </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">batch_index</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads_and_vars</span><span class="o">=</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition-cross-entropy-tf-keras-losses admonition">
<p class="admonition-title">交叉熵（cross entropy）與 <code class="docutils literal notranslate"><span class="pre">tf.keras.losses</span></code></p>
<p>你或許注意到了，在這裡，我們沒有明顯的寫出一個損失函數，而是使用了 <code class="docutils literal notranslate"><span class="pre">tf.keras.losses</span></code> 中的 <code class="docutils literal notranslate"><span class="pre">sparse_categorical_crossentropy</span></code> （交叉熵）函數，將模型的預測值 <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> 與真實的標籤值 <code class="docutils literal notranslate"><span class="pre">y</span></code> 作為函數參數傳入，由 Keras 幫助我們計算損失函數的值。</p>
<p>交叉熵作為損失函數，在分類問題中被廣泛應用。其離散形式為 <img class="math" src="../../_images/math/9ccb8a0722d5b80deb3aee2ce48b479e6ec889af.png" alt="H(y, \hat{y}) = -\sum_{i=1}^{n}y_i \log(\hat{y_i})"/> ，其中 <img class="math" src="../../_images/math/841e21d7600cefc9c7d41a5a9eb0e27b01d2e98e.png" alt="y"/> 為真實機率分佈， <img class="math" src="../../_images/math/a7d02c311e7fdb108e2153da67397cde3dc17965.png" alt="\hat{y}"/> 為預測機率分佈， <img class="math" src="../../_images/math/1005bf222658283b2edeaaaed3761ce5d5bb3e6c.png" alt="n"/> 為分類任務的類別個數。預測機率分佈與真實分佈越接近，則交叉熵的值越小，反之則越大。更具體的介紹及其在機器學習中的應用可參考 <a class="reference external" href="https://medium.com/&#64;chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9-%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8-loss-function-2dcac5ebb6cb">這篇文章</a> 。</p>
<p>在 <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> 中，有兩個交叉熵相關的損失函數 <code class="docutils literal notranslate"><span class="pre">tf.keras.losses.categorical_crossentropy</span></code> 和 <code class="docutils literal notranslate"><span class="pre">tf.keras.losses.sparse_categorical_crossentropy</span></code> 。其中 sparse 的含義是，真實的標籤值 <code class="docutils literal notranslate"><span class="pre">y_true</span></code> 可以直接傳入 int 類型的標籤類別。具體而言：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
<p>與</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">categorical_crossentropy</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
    <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span>
<span class="p">)</span>
</pre></div>
</div>
<p>的結果相同。</p>
</div>
</div>
<div class="section" id="tf-keras-metrics">
<h3>模型的評估： <code class="docutils literal notranslate"><span class="pre">tf.keras.metrics</span></code><a class="headerlink" href="#tf-keras-metrics" title="永久链接至标题">¶</a></h3>
<p>最後，我們使用測試集評估模型的性能。這裡，我們使用 <code class="docutils literal notranslate"><span class="pre">tf.keras.metrics</span></code> 中的 <code class="docutils literal notranslate"><span class="pre">SparseCategoricalAccuracy</span></code> 評量器來評估模型在測試集上的性能，該評量器能夠對模型預測的結果與真實結果進行比較，並輸出預測正確的樣本數占總樣本數的比例。我們疊代測試資料集，每次通過 <code class="docutils literal notranslate"><span class="pre">update_state()</span></code> 方法向評量器輸入兩個參數： <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> 和 <code class="docutils literal notranslate"><span class="pre">y_true</span></code> ，即模型預測出的結果和真實結果。評量器具有內部變數來保存當前評估指標相關的參數數值（例如當前已傳入的累計樣本數和當前預測正確的樣本數）。疊代結束後，我們使用 <code class="docutils literal notranslate"><span class="pre">result()</span></code> 方法輸出最終的評量指標值（預測正確的樣本數占總樣本數的比例）。</p>
<p>在以下評量器程式碼中，我們提出了一個實例 <code class="docutils literal notranslate"><span class="pre">tf.keras.metrics.SparseCategoricalAccuracy</span></code>，並使用 For 循環疊代分批次傳入了測試集資料的預測結果與真實結果，並輸出訓練後的模型在測試資料集上的準確率。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">sparse_categorical_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">()</span>
    <span class="n">num_batches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">data_loader</span><span class="o">.</span><span class="n">num_test_data</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">batch_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span>
        <span class="n">start_index</span><span class="p">,</span> <span class="n">end_index</span> <span class="o">=</span> <span class="n">batch_index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data_loader</span><span class="o">.</span><span class="n">test_data</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span> <span class="n">end_index</span><span class="p">])</span>
        <span class="n">sparse_categorical_accuracy</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">data_loader</span><span class="o">.</span><span class="n">test_label</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span> <span class="n">end_index</span><span class="p">],</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test accuracy: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_categorical_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">())</span>
</pre></div>
</div>
<p>輸出結果:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.947900</span>
</pre></div>
</div>
<p>可以注意到，使用這樣簡單的模型，已經可以達到 95% 左右的準確率。</p>
<div class="admonition-order admonition" id="neuron">
<p class="admonition-title">神經網路的基本單位：神經元 <a class="footnote-reference brackets" href="#order" id="id9">3</a></p>
<p>如果我們將上面的神經網路放大來看，詳細研究計算過程，比如取第二層的第 k 個計算單元，可以得到示意圖如下：</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/neuron.png"><img alt="../../_images/neuron.png" src="../../_images/neuron.png" style="width: 80%;" /></a>
</div>
<p>該計算單元 <img class="math" src="../../_images/math/85ce41d6dc4430e698ee09025e20ef92dc072c8b.png" alt="Q_k"/> 有 100 個權重值參數 <img class="math" src="../../_images/math/367b9541701e95dc3ba7576de1d9f73d4b503756.png" alt="w_{0k}, w_{1k}, ..., w_{99k}"/> 和 1 個偏移參數 <img class="math" src="../../_images/math/d7045b226648b5f10f66aee02be62f28ffada7a2.png" alt="b_k"/> 。將第 1 層中所有的 100 個計算單元 <img class="math" src="../../_images/math/6d9bbcc666002052dfb6ebca8f6d27e96b922abb.png" alt="P_0, P_1, ..., P_{99}"/> 的值作為輸入，分別按權重值 <img class="math" src="../../_images/math/9c31ad5764fc67b721c08d9b53883c8504ec39a6.png" alt="w_{ik}"/> 加和（即 <img class="math" src="../../_images/math/ea226ce2a26a7170cdd886bcc69bbd2c2173a0a2.png" alt="\sum_{i=0}^{99} w_{ik} P_i"/> ），並加上偏置值 <img class="math" src="../../_images/math/d7045b226648b5f10f66aee02be62f28ffada7a2.png" alt="b_k"/> ，然後送入激活函數 <img class="math" src="../../_images/math/95dc71c95728695dd2fada939d146ae97def5061.png" alt="f"/> 進行計算，即得到輸出結果。</p>
<p>事實上，這種結構和真實的神經細胞（神經元）類似。神經元由樹突、細胞體和軸突構成。樹突接受其他神經元傳來的信號作為輸入（一個神經元可以有數千甚至上萬樹突），細胞體對電位信號進行整合，而產生的信號則通過軸突傳到神經末梢的突觸，傳播到下一個（或多個）神經元。</p>
<div class="figure align-center" id="id29">
<a class="reference internal image-reference" href="../../_images/real_neuron_cn.png"><img alt="../../_images/real_neuron_cn.png" src="../../_images/real_neuron_cn.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-text">神經細胞模式圖（修改自 Quasar Jarosz at English Wikipedia [CC BY-SA 3.0 (<a class="reference external" href="https://creativecommons.org/licenses/by-sa/3.0">https://creativecommons.org/licenses/by-sa/3.0</a>)]）</span><a class="headerlink" href="#id29" title="永久链接至图片">¶</a></p>
</div>
<p>上面的計算單元，可以被視作對神經元結構的數學建模。在上面的例子裡，第二層的每一個計算單元（神經元）有 100 個權重值參數和 1 個偏移參數，而第二層計算單元的數目是 10 個，因此這一個全連接層的總參數量為 100*10 個權重值參數和 10 個偏移參數。事實上，這正是該全連接層中的兩個變數 <code class="docutils literal notranslate"><span class="pre">kernel</span></code> 和 <code class="docutils literal notranslate"><span class="pre">bias</span></code> 的形狀。仔細研究一下，你會發現，這裡基於神經元建模的介紹與上文基於矩陣計算的介紹是等價的。</p>
<dl class="footnote brackets">
<dt class="label" id="order"><span class="brackets"><a class="fn-backref" href="#id9">3</a></span></dt>
<dd><p>事實上，應當是先有神經元建模的概念，再有基於神經元和層結構的神經網路。但由於本手冊著重介紹 TensorFlow 的使用方法，所以調整了介紹順序。</p>
</dd>
</dl>
</div>
</div>
</div>
<div class="section" id="cnn">
<h2>卷積神經網路（CNN）<a class="headerlink" href="#cnn" title="永久链接至标题">¶</a></h2>
<p><a class="reference external" href="https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">卷積神經網路</a> （Convolutional Neural Network, CNN）是一種結構類似於人類或動物的 <a class="reference external" href="https://zh.wikipedia.org/wiki/%E8%A7%86%E8%A7%89%E7%B3%BB%E7%BB%9F">視覺系統</a> 的神經網路，包含一個或多個卷積層（Convolutional Layer）、池化層（Pooling Layer）和全連接層（Fully-connected Layer）。</p>
<div class="admonition- admonition">
<p class="admonition-title">基礎知識和原理</p>
<ul class="simple">
<li><p>台灣大學李宏毅教授的《機器學習》課程的 <a class="reference external" href="https://www.youtube.com/watch?v=FrKWiRv254g">Convolutional Neural Network</a> 一章；</p></li>
<li><p>UFLDL 教程 <a class="reference external" href="http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/">Convolutional Neural Network</a> 一節；</p></li>
<li><p>史丹佛課程 <a class="reference external" href="https://cs231n.github.io/">CS231n: Convolutional Neural Networks for Visual Recognition</a> 中的 “Module 2: Convolutional Neural Networks” 部分。</p></li>
</ul>
</div>
<div class="section" id="keras">
<h3>使用Keras實現卷積神經網路<a class="headerlink" href="#keras" title="永久链接至标题">¶</a></h3>
<p>卷積神經網路的一個範例實現如下所示，和 <a class="reference internal" href="#mlp-model"><span class="std std-ref">上節中的多層感知器</span></a> 在程式碼結構上很類似，只是新加入了一些卷積層和池化層。這裡的網路結構並不是唯一的，可以增加、刪除或調整 CNN 的網路結構和參數，以達到更好的性能。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
            <span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>             <span class="c1"># 卷積層神經元（卷積核）數目</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>     <span class="c1"># 接受區的大小</span>
            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>         <span class="c1"># padding策略（vaild 或 same）</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span>   <span class="c1"># 激活函数</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
            <span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">target_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">64</span><span class="p">,))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>                  <span class="c1"># [batch_size, 28, 28, 32]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                       <span class="c1"># [batch_size, 14, 14, 32]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                       <span class="c1"># [batch_size, 14, 14, 64]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                       <span class="c1"># [batch_size, 7, 7, 64]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                     <span class="c1"># [batch_size, 7 * 7 * 64]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                      <span class="c1"># [batch_size, 1024]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                      <span class="c1"># [batch_size, 10]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<div class="figure align-center" id="id30">
<img alt="../../_images/cnn.png" src="../../_images/cnn.png" />
<p class="caption"><span class="caption-text">範例程式碼中的 CNN 結構圖示</span><a class="headerlink" href="#id30" title="永久链接至图片">¶</a></p>
</div>
<p>將前節的 <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">MLP()</span></code> 更換成 <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">CNN()</span></code> ，輸出如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.988100</span>
</pre></div>
</div>
<p>可以發現準確率相較於前節的多層感知器有非常顯著的提高。事實上，通過改變模型的網路結構（比如加入 Dropout 層防止過擬合），準確率還有進一步提升的空間。</p>
</div>
<div class="section" id="id14">
<h3>使用Keras中預定義的典型卷積神經網路結構<a class="headerlink" href="#id14" title="永久链接至标题">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">tf.keras.applications</span></code> 中有一些預定義好的典型卷積神經網路結構，如 <code class="docutils literal notranslate"><span class="pre">VGG16</span></code> 、 <code class="docutils literal notranslate"><span class="pre">VGG19</span></code> 、 <code class="docutils literal notranslate"><span class="pre">ResNet</span></code> 、 <code class="docutils literal notranslate"><span class="pre">MobileNet</span></code> 等。我們可以直接呼叫這些典型的卷積神經網路結構（甚至載入預訓練的參數），而無需手動定義網路結構。</p>
<p>例如，我們可以使用以下代碼來實例化一個 <code class="docutils literal notranslate"><span class="pre">MobileNetV2</span></code> 網路結構：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">MobileNetV2</span><span class="p">()</span>
</pre></div>
</div>
<p>當執行以上程式碼時，TensorFlow會自動從網路上下載 <code class="docutils literal notranslate"><span class="pre">MobileNetV2</span></code> 網路結構，因此在第一次執行程式碼時需要具備網路連接。每個網路結構具有自己特定的詳細參數設置，一些共通的常用參數如下：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_shape</span></code> ：輸入張量的形狀（不含第一維的Batch），大多預設為 <code class="docutils literal notranslate"><span class="pre">224</span> <span class="pre">×</span> <span class="pre">224</span> <span class="pre">×</span> <span class="pre">3</span></code> 。一般而言，模型對輸入張量的大小有下限，長和寬至少為 <code class="docutils literal notranslate"><span class="pre">32</span> <span class="pre">×</span> <span class="pre">32</span></code> 或 <code class="docutils literal notranslate"><span class="pre">75</span> <span class="pre">×</span> <span class="pre">75</span></code> ；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">include_top</span></code> ：在網路的最後是否包含全連接層，默認為 <code class="docutils literal notranslate"><span class="pre">True</span></code> ；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weights</span></code> ：預訓練權重值，預設為 <code class="docutils literal notranslate"><span class="pre">'imagenet'</span></code> ，即為當前模型載入在ImageNet資料集上預訓練的權重值。如需隨機初始化變數可設為 <code class="docutils literal notranslate"><span class="pre">None</span></code> ；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">classes</span></code> ：分類數，預設為1000。修改該參數需要 <code class="docutils literal notranslate"><span class="pre">include_top</span></code> 參數為 <code class="docutils literal notranslate"><span class="pre">True</span></code> 且 <code class="docutils literal notranslate"><span class="pre">weights</span></code> 參數為 <code class="docutils literal notranslate"><span class="pre">None</span></code> 。</p></li>
</ul>
<p>各網路模型參數的詳細介紹可參考 <a class="reference external" href="https://keras.io/applications/">Keras文件</a> 。</p>
<p>    對於一些預定義的經典模型，其中的某些層（例如``BatchNormalization`` ）在訓練和測試時的行為是不同的（可以參考`這篇文章&lt;<a class="reference external" href="https://zhuanlan.zhihu.com/p/64310188">https://zhuanlan.zhihu.com/p/64310188</a>&gt;`_ ）。因此，在訓練模型時，需要手動設置訓練狀態，告訴模型“我現在是處於訓練模型的階段”。可以通過</p>
<p>    .. code-block:: python</p>
<p>        tf.keras.backend.set_learning_phase(True)</p>
<p>    進行設置，也可以在調用模型時通過將 <code class="docutils literal notranslate"><span class="pre">training</span></code> 參數設為 <code class="docutils literal notranslate"><span class="pre">True</span></code> 來設置。</p>
<p>以下展示一個例子，使用 <code class="docutils literal notranslate"><span class="pre">MobileNetV2</span></code> 網路在 <code class="docutils literal notranslate"><span class="pre">tf_flowers</span></code> 五種分類數據集上進行訓練（為了程式碼的簡短高效，在該範例中我們使用了 <a class="reference internal" href="../appendix/tfds.html"><span class="doc">TensorFlow Datasets</span></a> 和 <a class="reference internal" href="tools.html#tfdata"><span class="std std-ref">tf.data</span></a> 載入和預處理資料）。通過將 <code class="docutils literal notranslate"><span class="pre">weights</span></code> 設置為 <code class="docutils literal notranslate"><span class="pre">None</span></code> ，我們隨機初始化變數而不使用預訓練權重值。同時將 <code class="docutils literal notranslate"><span class="pre">classes</span></code> 設置為5，對應於5種分類的資料集。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>

<span class="n">num_epoch</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;tf_flowers&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="n">tfds</span><span class="o">.</span><span class="n">Split</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span> <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">img</span><span class="p">,</span> <span class="n">label</span><span class="p">:</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">1024</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="hll"><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">MobileNetV2</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epoch</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
<span class="hll">            <span class="n">labels_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">labels_pred</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loss </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads_and_vars</span><span class="o">=</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">labels_pred</span><span class="p">)</span>
</pre></div>
</div>
<p>後文的部分章節（如 <a class="reference internal" href="../appendix/distributed.html"><span class="doc">分散式訓練</span></a> ）中，我們也會直接呼叫這些經典的網路結構來進行訓練。</p>
<div class="admonition- admonition">
<p class="admonition-title">卷積層和池化層的工作原理</p>
<p>卷積層（Convolutional Layer，以 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Conv2D</span></code> 為代表）是 CNN 的核心組件，其結構與大腦的視覺皮層有相似之處。</p>
<p>回憶我們之前建立的 <a class="reference internal" href="#neuron"><span class="std std-ref">神經細胞的計算模型</span></a> 以及全連接層，我們預設每個神經元與上一層的所有神經元相連。不過，在視覺皮層的神經元中，情況並不是這樣。你或許在生物課上學習過 <strong>接受區</strong> （Receptive Field）這一概念，即視覺皮層中的神經元並非與前一層的所有神經元相連，而只是感受一片區域內的視覺信號，並只對局部區域的視覺刺激進行反應。CNN 中的卷積層正體現了這一特性。</p>
<p>例如，下圖是一個 7×7 的單通道圖片信號輸入：</p>
<div class="figure align-center">
<img alt="../../_images/conv_image.png" src="../../_images/conv_image.png" />
</div>
<p>如果使用之前基於全連接層的模型，我們需要讓每個輸入信號對應一個權重值，即建模一個神經元需要 7×7=49 個權重值（加上偏置項是50個），並得到一個輸出信號。如果一層有 N 個神經元，我們就需要 49N 個權重值，並得到 N 個輸出信號。</p>
<p>而在 CNN 的卷積層中，我們這樣建立一個卷積層的神經元：</p>
<div class="figure align-center">
<img alt="../../_images/conv_field.png" src="../../_images/conv_field.png" />
</div>
<p>圖中 3×3 的紅框代表該神經元的接受區。由此，我們只需 3×3=9 個權重值 <img class="math" src="../../_images/math/bee58dc59fed173d1b6f8651b4b48d2f48cf8273.png" alt="W = \begin{bmatrix}w_{1, 1} &amp; w_{1, 2} &amp; w_{1, 3} \\w_{2, 1} &amp; w_{2, 2} &amp; w_{2, 3} \\w_{3, 1} &amp; w_{3, 2} &amp; w_{3, 3}\end{bmatrix}"/>  ，外加1個偏移項 <img class="math" src="../../_images/math/3de2ed824326dc0eb383a42dd6ec44d3b401047b.png" alt="b"/>  ，即可得到一個輸出信號。例如，對於紅框所示的位置，輸出信號即為對矩陣 <img class="math" src="../../_images/math/a6bd64f44fe6aba75c5cf32b1122839630deeac4.png" alt="\begin{bmatrix}0 \times w_{1, 1} &amp; 0 \times w_{1, 2} &amp; 0 \times w_{1, 3} \\0 \times w_{2, 1} &amp; 1 \times w_{2, 2} &amp; 0 \times w_{2, 3} \\0 \times w_{3, 1} &amp; 0 \times w_{3, 2} &amp; 2 \times w_{3, 3}\end{bmatrix}"/> 的所有元素求和並加上偏移項 <img class="math" src="../../_images/math/3de2ed824326dc0eb383a42dd6ec44d3b401047b.png" alt="b"/>，記作 <img class="math" src="../../_images/math/da3be5884aa8c28078557f8095fb652851c1f54c.png" alt="a_{1, 1}"/>  。</p>
<p>不過，3×3 的範圍顯然不足以處理整個圖片，因此我們使用滑動視窗的方法。使用相同的參數 <img class="math" src="../../_images/math/eb33eca75f5d77363c991bf041953da54a858bfb.png" alt="W"/> ，但將紅框在圖片中從左到右滑動，進行逐行掃描，每滑動到一個位置就計算一個值。例如，當紅框向右移動一個單位時，我們計算矩陣 <img class="math" src="../../_images/math/7b13de07ea927ebc0506bbad56c31fe76959bb69.png" alt="\begin{bmatrix}0 \times w_{1, 1} &amp; 0 \times w_{1, 2} &amp; 0 \times w_{1, 3} \\1 \times w_{2, 1} &amp; 0 \times w_{2, 2} &amp; 1 \times w_{2, 3} \\0 \times w_{3, 1} &amp; 2 \times w_{3, 2} &amp; 1 \times w_{3, 3}\end{bmatrix}"/> 的所有元素的和加上偏移項 <img class="math" src="../../_images/math/3de2ed824326dc0eb383a42dd6ec44d3b401047b.png" alt="b"/>，記作 <img class="math" src="../../_images/math/396ef03019e9f0bb2c3c7d1cafc1101248eef70a.png" alt="a_{1, 2}"/> 。由此，和一般的神經元只能輸出 1 個值不同，這里的卷積層神經元可以輸出一個 5×5 的矩陣 <img class="math" src="../../_images/math/a402f89ed5d1adab75c2ae66287ea2697ef01e0f.png" alt="A = \begin{bmatrix}a_{1, 1} &amp; \cdots &amp; a_{1, 5} \\ \vdots &amp; &amp; \vdots \\ a_{5, 1} &amp; \cdots &amp; a_{5, 5}\end{bmatrix}"/>  。</p>
<div class="figure align-center" id="id31">
<img alt="../../_images/conv_procedure_cn.png" src="../../_images/conv_procedure_cn.png" />
<p class="caption"><span class="caption-text">卷積示意圖。一個單通道的 7×7 圖片在通過一個接受區為 3×3 ，參數為10個的卷積層神經元後，得到 5×5 的矩陣作為卷積結果。</span><a class="headerlink" href="#id31" title="永久链接至图片">¶</a></p>
</div>
<p>下面，我們使用TensorFlow來驗證一下上圖的計算結果。</p>
<p>將上圖中的輸入圖片、權重值矩陣 <img class="math" src="../../_images/math/eb33eca75f5d77363c991bf041953da54a858bfb.png" alt="W"/> 和偏移項 <img class="math" src="../../_images/math/3de2ed824326dc0eb383a42dd6ec44d3b401047b.png" alt="b"/> 表示為NumPy陣列 <code class="docutils literal notranslate"><span class="pre">image</span></code> , <code class="docutils literal notranslate"><span class="pre">W</span></code> , <code class="docutils literal notranslate"><span class="pre">b</span></code> 如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># TensorFlow 的圖片表示為 [圖片數目，長，寬，色彩通道數] 的四維張量</span>
<span class="c1"># 這裡我們的輸入圖片 image 的張量形狀為 [1, 7, 7, 1]</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span>
    <span class="p">[</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> 
    <span class="p">[</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span> <span class="p">],</span> 
    <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="p">]</span>
<span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
<p>然後建立一個僅有一個卷積層的模型，用 <code class="docutils literal notranslate"><span class="pre">W</span></code> 和 <code class="docutils literal notranslate"><span class="pre">b</span></code> 初始化 <a class="footnote-reference brackets" href="#sequential" id="id16">4</a> ：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>              <span class="c1"># 卷積層神經元（卷積核）數目</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>     <span class="c1"># 接受區大小</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="n">W</span><span class="p">),</span>
        <span class="n">bias_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="p">)]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>最後將圖片資料 <code class="docutils literal notranslate"><span class="pre">image</span></code> 輸入模型，列印輸出：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
</pre></div>
</div>
<p>程式運行結果為：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
<span class="p">[[</span> <span class="mf">6.</span>  <span class="mf">5.</span> <span class="o">-</span><span class="mf">2.</span>  <span class="mf">1.</span>  <span class="mf">2.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">3.</span>  <span class="mf">0.</span>  <span class="mf">3.</span>  <span class="mf">2.</span> <span class="o">-</span><span class="mf">2.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">4.</span>  <span class="mf">2.</span> <span class="o">-</span><span class="mf">1.</span>  <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">2.</span>  <span class="mf">1.</span>  <span class="mf">2.</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">-</span><span class="mf">3.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">3.</span>  <span class="mf">1.</span><span class="p">]],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
<p>可見與上圖中矩陣 <img class="math" src="../../_images/math/b86d9659948ffa756133d580cfc95f923705c2b5.png" alt="A"/>  的值一致。</p>
<p>還有一個問題，以上假設圖片都只有一個通道（例如灰階圖片），但如果圖片是彩色的（例如有 RGB 三個通道）該怎麼辦呢？此時，我們可以為每個通道準備一個 3×3 的權重值矩陣，即一共有 3×3×3=27 個權重值。對於每個通道，均使用自己的權重值矩陣進行處理，輸出時將多個通道所輸出的值進行加和即可。</p>
<p>可能有讀者會注意到，按照上述介紹的方法，每次卷積後的結果相比於原始圖片而言，四周都會“少一圈”。比如上面 7×7 的圖片，卷積後變成了 5×5 ，這有時會為後面的工作帶來麻煩。因此，我們可以設定padding策略。在 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Conv2D</span></code> 中，當我們將 <code class="docutils literal notranslate"><span class="pre">padding</span></code> 參數設為 <code class="docutils literal notranslate"><span class="pre">same</span></code> 時，會將周圍缺少的部分使用0補齊，使得輸出的矩陣大小和輸入一致。</p>
<p>最後，既然我們可以使用滑動窗口的方法進行卷積，那麼每次滑動的步長是不是可以設置呢？答案是肯定的。通過 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Conv2D</span></code> 的 <code class="docutils literal notranslate"><span class="pre">strides</span></code> 參數即可設置步長（預設為1）。比如，在上面的例子中，如果我們將步長設定為2，輸出的卷積結果即會是一個3×3的矩陣。</p>
<p>事實上，卷積的形式多種多樣，以上的介紹只是其中最簡單和基礎的一種。更多卷積方式的範例可見 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic">Convolution arithmetic</a> 。</p>
<p>池化層（Pooling Layer）的理解則簡單得多，其可以理解為對圖片進行下降取樣的過程，對於每一次滑動窗口中的所有值，輸出其中的最大值（MaxPooling）、均值或其他方法產生的值。例如，對於一個三通道的 16×16 圖像（即一個 <code class="docutils literal notranslate"><span class="pre">16*16*3</span></code> 的張量），經過感受野為 2×2，滑動步長為 2 的池化層，則得到一個 <code class="docutils literal notranslate"><span class="pre">8*8*3</span></code> 的張量。</p>
<dl class="footnote brackets">
<dt class="label" id="sequential"><span class="brackets"><a class="fn-backref" href="#id16">4</a></span></dt>
<dd><p>這里使用了較為簡易的Sequential模式建立模型，具體介紹見 <a class="reference internal" href="#sequential-functional"><span class="std std-ref">後文</span></a>  。</p>
</dd>
</dl>
</div>
</div>
</div>
<div class="section" id="rnn">
<h2>循環神經網路（RNN）<a class="headerlink" href="#rnn" title="永久链接至标题">¶</a></h2>
<p>循環神經網路（Recurrent Neural Network, RNN）是一種適宜於處理序列資料的神經網路，被廣泛用於語言模型、文本生成、機器翻譯等。</p>
<div class="admonition- admonition">
<p class="admonition-title">基礎知識和原理</p>
<ul class="simple">
<li><p><a class="reference external" href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs</a></p></li>
<li><p>台灣大學李宏毅教授的《機器學習》課程的 <a class="reference external" href="https://www.youtube.com/watch?v=xCGidAeyS4M">Recurrent Neural Network (part 1)</a> <a class="reference external" href="https://www.youtube.com/watch?v=rTqmWlnwz_0">Recurrent Neural Network (part 2)</a> 兩部分。</p></li>
<li><p>LSTM 原理：<a class="reference external" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></p></li>
<li><p>RNN 序列生成：<a class="reference internal" href="#graves2013" id="id17"><span>[Graves2013]</span></a></p></li>
</ul>
</div>
<p>這裡，我們使用 RNN 來進行尼采風格文本的自動生成。 <a class="footnote-reference brackets" href="#rnn-reference" id="id18">5</a></p>
<p>這個任務的本質其實預測一段英文文本的接續字母的機率分佈。比如，我們有以下句子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">I</span> <span class="n">am</span> <span class="n">a</span> <span class="n">studen</span>
</pre></div>
</div>
<p>這個句子（序列）一共有 13 個字符（包含空格）。當我們閱讀到這個由 13 個字符組成的序列後，根據我們的經驗，我們可以預測出下一個字符很大機率是 “t”。我們希望建立這樣一個模型，逐個輸入一段長為 <code class="docutils literal notranslate"><span class="pre">seq_length</span></code> 的序列，輸出這些序列接續的下一個字元的機率分佈。我們從下一個字符的機率分佈中取樣作為預測值，然後滾雪球式的生成下兩個字符，下三個字符等等，即可完成文本的生成任務。</p>
<p>首先，還是實現一個簡單的 <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 類別來讀取文本，並以字符為單位進行編碼。設字符種類數為 <code class="docutils literal notranslate"><span class="pre">num_chars</span></code> ，則每種字符賦予一個 0 到 <code class="docutils literal notranslate"><span class="pre">num_chars</span> <span class="pre">-</span> <span class="pre">1</span></code> 之間的唯一整數編號 i。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DataLoader</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_file</span><span class="p">(</span><span class="s1">&#39;nietzsche.txt&#39;</span><span class="p">,</span>
            <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;https://s3.amazonaws.com/text-datasets/nietzsche.txt&#39;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">raw_text</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">raw_text</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">char_indices</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">c</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices_char</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">char_indices</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">raw_text</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">next_char</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text</span><span class="p">)</span> <span class="o">-</span> <span class="n">seq_length</span><span class="p">)</span>
            <span class="n">seq</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="n">index</span><span class="p">:</span><span class="n">index</span><span class="o">+</span><span class="n">seq_length</span><span class="p">])</span>
            <span class="n">next_char</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="n">index</span><span class="o">+</span><span class="n">seq_length</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">seq</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">next_char</span><span class="p">)</span>       <span class="c1"># [batch_size, seq_length], [num_batch]</span>
</pre></div>
</div>
<p>接下來進行模型的實現。在 <code class="docutils literal notranslate"><span class="pre">__init__</span></code> 方法中我們實例化一個常用的 <code class="docutils literal notranslate"><span class="pre">LSTMCell</span></code> 單元，以及一個線性變換用的全連接層，我們首先對序列進行“One Hot”操作，即將序列中的每個字元的編碼 i 均變換為一個 <code class="docutils literal notranslate"><span class="pre">num_char</span></code> 維向量，其第 i 位為 1，其餘均為 0。變換後的序列張量形狀為 <code class="docutils literal notranslate"><span class="pre">[seq_length,</span> <span class="pre">num_chars]</span></code> 。然後，我們初始化 RNN 單元的狀態，存入變量 <code class="docutils literal notranslate"><span class="pre">state</span></code> 中。接下來，將序列從頭到尾依次送入 RNN 單元，即在 t 時間，將上一個時間 t-1 的 RNN 單元狀態 <code class="docutils literal notranslate"><span class="pre">state</span></code> 和序列的第 t 個元素 <code class="docutils literal notranslate"><span class="pre">inputs[t,</span> <span class="pre">:]</span></code> 送入 RNN 單元，得到當前時間的輸出 <code class="docutils literal notranslate"><span class="pre">output</span></code> 和 RNN 單元狀態。取 RNN 單元最後一次的輸出，通過全連接層變換到 <code class="docutils literal notranslate"><span class="pre">num_chars</span></code> 維，即作為模型的輸出。</p>
<div class="figure align-center" id="id32">
<a class="reference internal image-reference" href="../../_images/rnn_single.jpg"><img alt="../../_images/rnn_single.jpg" src="../../_images/rnn_single.jpg" style="width: 50%;" /></a>
<p class="caption"><span class="caption-text"><code class="docutils literal notranslate"><span class="pre">output,</span> <span class="pre">state</span> <span class="pre">=</span> <span class="pre">self.cell(inputs[:,</span> <span class="pre">t,</span> <span class="pre">:],</span> <span class="pre">state)</span></code> 圖示</span><a class="headerlink" href="#id32" title="永久链接至图片">¶</a></p>
</div>
<div class="figure align-center" id="id33">
<a class="reference internal image-reference" href="../../_images/rnn.jpg"><img alt="../../_images/rnn.jpg" src="../../_images/rnn.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">RNN 流程圖示</span><a class="headerlink" href="#id33" title="永久链接至图片">¶</a></p>
</div>
<p>具體實現如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_chars</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_chars</span> <span class="o">=</span> <span class="n">num_chars</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_chars</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_chars</span><span class="p">)</span>       <span class="c1"># [batch_size, seq_length, num_chars]</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">get_initial_state</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">):</span>
            <span class="n">output</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:],</span> <span class="n">state</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">from_logits</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">logits</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
</pre></div>
</div>
<p>定義一些模型超參數：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">num_batches</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">seq_length</span> <span class="o">=</span> <span class="mi">40</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
</pre></div>
</div>
<p>訓練過程與前節基本一致，在此不再贅述：</p>
<ul class="simple">
<li><p>從 <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 中隨機取一批訓練資料；</p></li>
<li><p>將這批資料送入模型，計算出模型的預測值；</p></li>
<li><p>將模型預測值與真實值進行比較，計算損失函數（loss）；</p></li>
<li><p>計算損失函數關於模型變數的導數；</p></li>
<li><p>使用優化器更新模型參數以最小化損失函數。</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">num_chars</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="o">.</span><span class="n">chars</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">batch_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data_loader</span><span class="o">.</span><span class="n">get_batch</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;batch </span><span class="si">%d</span><span class="s2">: loss </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">batch_index</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads_and_vars</span><span class="o">=</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">))</span>
</pre></div>
</div>
<p>關於文本生成的過程有一點需要特別注意。之前，我們一直使用 <code class="docutils literal notranslate"><span class="pre">tf.argmax()</span></code> 函數，將對應機率最大的值作為預測值。然而對於文本生成而言，這樣的預測方式過於絕對，會使得生成的文本失去豐富性。於是，我們使用 <code class="docutils literal notranslate"><span class="pre">np.random.choice()</span></code> 函數按照生成的機率分佈取樣。這樣，即使是對應機率較小的字元，也有機會被取樣到。同時，我們加入一個 <code class="docutils literal notranslate"><span class="pre">temperature</span></code> 參數控制分佈的形狀，參數值越大則分佈越平緩（最大值和最小值的差值越小），生成文本的豐富度越高；參數值越小則分佈越陡峭，生成文本的豐富度越低。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_chars</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">prob</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
                         <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="o">.</span><span class="n">numpy</span><span class="p">())])</span>
</pre></div>
</div>
<p>通過這種方式進行 “滾雪球” 式的連續預測，即可得到生成文本。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">X_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">data_loader</span><span class="o">.</span><span class="n">get_batch</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">diversity</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X_</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;diversity </span><span class="si">%f</span><span class="s2">:&quot;</span> <span class="o">%</span> <span class="n">diversity</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">400</span><span class="p">):</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">diversity</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">data_loader</span><span class="o">.</span><span class="n">indices_char</span><span class="p">[</span><span class="n">y_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>生成的文本如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>diversity 0.200000:
conserted and conseive to the conterned to it is a self--and seast and the selfes as a seast the expecience and and and the self--and the sered is a the enderself and the sersed and as a the concertion of the series of the self in the self--and the serse and and the seried enes and seast and the sense and the eadure to the self and the present and as a to the self--and the seligious and the enders

diversity 0.500000:
can is reast to as a seligut and the complesed
has fool which the self as it is a the beasing and us immery and seese for entoured underself of the seless and the sired a mears and everyther to out every sone thes and reapres and seralise as a streed liees of the serse to pease the cersess of the selung the elie one of the were as we and man one were perser has persines and conceity of all self-el

diversity 1.000000:
entoles by
their lisevers de weltaale, arh pesylmered, and so jejurted count have foursies as is
descinty iamo; to semplization refold, we dancey or theicks-welf--atolitious on his
such which
here
oth idey of pire master, ie gerw their endwit in ids, is an trees constenved mase commars is leed mad decemshime to the mor the elige. the fedies (byun their ope wopperfitious--antile and the it as the f

diversity 1.200000:
cain, elvotidue, madehoublesily
inselfy!--ie the rads incults of to prusely le]enfes patuateded:.--a coud--theiritibaior &quot;nrallysengleswout peessparify oonsgoscess teemind thenry ansken suprerial mus, cigitioum: 4reas. whouph: who
eved
arn inneves to sya&quot; natorne. hag open reals whicame oderedte,[fingo is
zisternethta simalfule dereeg hesls lang-lyes thas quiin turjentimy; periaspedey tomm--whach
</pre></div>
</div>
<dl class="footnote brackets">
<dt class="label" id="rnn-reference"><span class="brackets"><a class="fn-backref" href="#id18">5</a></span></dt>
<dd><p>此處的任務及實現參考了 <a class="reference external" href="https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py">https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py</a></p>
</dd>
</dl>
<div class="admonition- admonition">
<p class="admonition-title">循環神經網路的工作過程</p>
<p>循環神經網路是一個處理時間序列資料的神經網路結構，也就是說，我們需要在腦海裡有一根時間軸，循環神經網路具有初始狀態 <img class="math" src="../../_images/math/5792932d14af4ccbd3739af3d66e5ee35e05dffc.png" alt="s_0"/> ，在每個時間點 <img class="math" src="../../_images/math/657fd8f3c80e7a724ba99cda423f58cff075f07b.png" alt="t"/> 迭代對當前時間的輸入 <img class="math" src="../../_images/math/80e7212143003eaa2efb31fc9c7aa8f016c3a3dd.png" alt="x_t"/> 進行處理，修改自身的狀態 <img class="math" src="../../_images/math/aef454ab9a36bb7c18fa5c3ba6a5b1523c56ddd5.png" alt="s_t"/> ，並進行輸出 <img class="math" src="../../_images/math/1dcf8af77e86ef764397a8f107aa550c617351c4.png" alt="o_t"/> 。</p>
<p>循環神經網路的核心是狀態 <img class="math" src="../../_images/math/bafbf63de17508c8c25cd882e0178342c193ae0c.png" alt="s"/> ，是一個特定維數的向量，類似於神經網路的 “記憶”。在 <img class="math" src="../../_images/math/f7158208ef875a964aba3a1e6c8b28c59f39682d.png" alt="t=0"/> 的初始時刻，<img class="math" src="../../_images/math/5792932d14af4ccbd3739af3d66e5ee35e05dffc.png" alt="s_0"/> 被賦予一個初始值（常用的為全 0 向量）。然後，我們用類似於遞歸的方法來描述循環神經網路的工作過程。即在 <img class="math" src="../../_images/math/657fd8f3c80e7a724ba99cda423f58cff075f07b.png" alt="t"/> 時間，我們假設 <img class="math" src="../../_images/math/2613231964fb187e75df4c2700ce50c0af85b9c7.png" alt="s_{t-1}"/> 已經求出，注意如何在此基礎上求出 <img class="math" src="../../_images/math/fc20b335e1d86d0a4ea29849968644207c7661c1.png" alt="s_{t}"/> ：</p>
<ul class="simple">
<li><p>對輸入向量 <img class="math" src="../../_images/math/80e7212143003eaa2efb31fc9c7aa8f016c3a3dd.png" alt="x_t"/> 通過矩陣 <img class="math" src="../../_images/math/0212136c77d0621b4e7c4525952b855f06e6afe8.png" alt="U"/> 進行線性變換，<img class="math" src="../../_images/math/bc5f463af6d8c26b7244d21b9659675e3e564556.png" alt="U x_t"/> 與狀態 s 具有相同的維度；</p></li>
<li><p>對 <img class="math" src="../../_images/math/2613231964fb187e75df4c2700ce50c0af85b9c7.png" alt="s_{t-1}"/> 通過矩陣 <img class="math" src="../../_images/math/eb33eca75f5d77363c991bf041953da54a858bfb.png" alt="W"/> 進行線性變換，<img class="math" src="../../_images/math/a5a5c143362f2787bebacc7d4956a94140a868d9.png" alt="W s_{t-1}"/> 與狀態 s 具有相同的維度；</p></li>
<li><p>將上述得到的兩個向量相加並通過激活函數，作為當前狀態 <img class="math" src="../../_images/math/aef454ab9a36bb7c18fa5c3ba6a5b1523c56ddd5.png" alt="s_t"/> 的值，即 <img class="math" src="../../_images/math/0c01ef2b58de660764f869ab39361569ccd789eb.png" alt="s_t = f(U x_t + W s_{t-1})"/>。也就是說，當前狀態的值是上一個狀態的值和當前輸入進行某種資訊整合而產生的；</p></li>
<li><p>對當前狀態 <img class="math" src="../../_images/math/aef454ab9a36bb7c18fa5c3ba6a5b1523c56ddd5.png" alt="s_t"/> 通過矩陣 <img class="math" src="../../_images/math/48d9710a8a12a6209e54e4d872252e9403e78683.png" alt="V"/> 進行線性變換，得到當前時間的輸出 <img class="math" src="../../_images/math/1dcf8af77e86ef764397a8f107aa550c617351c4.png" alt="o_t"/>。</p></li>
</ul>
<div class="figure align-center" id="id34">
<img alt="../../_images/rnn_cell.jpg" src="../../_images/rnn_cell.jpg" />
<p class="caption"><span class="caption-text">RNN 工作過程圖示（來自 <a class="reference external" href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/</a>）</span><a class="headerlink" href="#id34" title="永久链接至图片">¶</a></p>
</div>
<p>我們假設輸入向量 <img class="math" src="../../_images/math/80e7212143003eaa2efb31fc9c7aa8f016c3a3dd.png" alt="x_t"/> 、狀態 <img class="math" src="../../_images/math/bafbf63de17508c8c25cd882e0178342c193ae0c.png" alt="s"/> 和輸出向量 <img class="math" src="../../_images/math/1dcf8af77e86ef764397a8f107aa550c617351c4.png" alt="o_t"/> 的維度分別為 <img class="math" src="../../_images/math/4b59fb8246fb77698911d761c596ef6066458b4c.png" alt="m"/>、<img class="math" src="../../_images/math/1005bf222658283b2edeaaaed3761ce5d5bb3e6c.png" alt="n"/>、<img class="math" src="../../_images/math/8f788889a02d716315011e6c07d3b81cc1cfc419.png" alt="p"/>，則 <img class="math" src="../../_images/math/190195050cecc54ea0ac8083ae0aa6df0b02cf51.png" alt="U \in \mathbb{R}^{m \times n}"/>、<img class="math" src="../../_images/math/c71f85f8dbe403bd5b3d62f4b6f5d5f4544eb8f3.png" alt="W \in \mathbb{R}^{n \times n}"/>、<img class="math" src="../../_images/math/1c78f0e6e369155ac8d9d7f2092f6617dee6c494.png" alt="V \in \mathbb{R}^{n \times p}"/>。</p>
<p>上述為最基礎的 RNN 原理介紹。在實際使用時往往使用一些常見的改進型，如LSTM（長短期記憶神經網路，解決了長序列的梯度消失問題，適用於較長的序列）、GRU等。</p>
</div>
</div>
<div class="section" id="drl">
<span id="zh-hant-rl"></span><h2>深度強化學習（DRL）<a class="headerlink" href="#drl" title="永久链接至标题">¶</a></h2>
<p><a class="reference external" href="https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0">強化學習</a> （Reinforcement learning，RL）強調如何基於環境而行動，以取得最大化的預期利益。結合了深度學習技術後的強化學習（Deep Reinforcement learning，DRL）更是如虎添翼。近年廣為人知的 AlphaGo 即是深度強化學習的典型應用。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>可參考本手冊附錄的 :強化學習簡介 <a class="reference internal" href="../appendix/rl.html"><span class="doc">強化學習簡介</span></a> 一章以獲得強化學習的基礎知識。</p>
</div>
<p>這裡，我們使用深度強化學習玩 CartPole（平衡桿）遊戲。平衡桿是控制論中的經典問題，在這個遊戲中，一根桿子的底部與一個小車通過軸相連，而桿的重心在軸之上，因此是一個不穩定的系統。在重力的作用下，桿很容易倒下。而我們則需要控制小車在水平的軌道上進行左右運動，以使得桿一直保持豎直平衡狀態。</p>
<div class="figure align-center" id="id35">
<a class="reference internal image-reference" href="../../_images/cartpole.gif"><img alt="../../_images/cartpole.gif" src="../../_images/cartpole.gif" style="width: 500px;" /></a>
<p class="caption"><span class="caption-text">CartPole 遊戲</span><a class="headerlink" href="#id35" title="永久链接至图片">¶</a></p>
</div>
<p>我們使用 <a class="reference external" href="https://gym.openai.com/">OpenAI 推出的 Gym 環境套件</a> 中的 CartPole 遊戲環境，可使用 <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">gym</span></code> 進行安裝，具體安裝步驟和教程可參考 <a class="reference external" href="https://gym.openai.com/docs/">官方文件</a> 和 <a class="reference external" href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-4-gym/">這裡</a> 。和Gym的交互過程很像是一個回合制遊戲，我們首先獲得遊戲的初始狀態（比如桿子的初始角度和小車位置），然後在每個回合t，我們都需要在當前可行的動作中選擇一個並交由Gym執行（比如向左或者向右推動小車，每個回合中二者只能擇一），Gym在執行動作後，會返回動作執行後的下一個狀態和當前回合所獲得的獎勵值（比如我們選擇向左推動小車並執行後，小車位置更加偏左，而桿的角度更加偏右，Gym將新的角度和位置返回給我們。而如果桿在這一回合仍沒有倒下，Gym同時返回給我們一個小的正獎勵）。這個過程可以一直疊代下去，直到遊戲結束（比如桿子倒下）。在 Python 中，Gym 的基本呼叫方法如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">)</span>       <span class="c1"># 實例化一個遊戲環境，參數為遊戲名稱</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>                 <span class="c1"># 初始化環境，獲得初始狀態</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>                    <span class="c1"># 對當前影像進行渲染，繪圖到螢幕</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>   <span class="c1"># 假設我們有一個訓練好的模型，能夠通過當前狀態預測出這時應該進行的動作</span>
    <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>   <span class="c1"># 讓環境執行動作，獲得執行完動作的下一個狀態，動作的獎勵，遊戲是否已結束以及額外資訊</span>
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>                        <span class="c1"># 如果遊戲結束則退出循環</span>
        <span class="k">break</span>
</pre></div>
</div>
<p>那麼，我們的任務就是訓練出一個模型，能夠根據當前的狀態預測出應該進行的一個好的動作。大致上來說，一個好的動作應當能夠最大化整個遊戲過程中獲得的獎勵之和，這也是強化學習的目標。以CartPole遊戲為例，我們的目標是希望做出合適的動作使得桿一直不倒，即遊戲交互的回合數盡可能地多。而回合每進行一次，我們都會獲得一個小的正獎勵，回合數越多則累積的獎勵值也越高。因此，我們最大化遊戲過程中的獎勵之和與我們的最終目標是一致的。</p>
<p>以下程式碼展示了如何使用深度強化學習中的 Deep Q-Learning 方法 <a class="reference internal" href="#mnih2013" id="id22"><span>[Mnih2013]</span></a> 來訓練模型。首先，我們引入TensorFlow、Gym和一些常用套件，並定義一些模型超參數：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">500</span>              <span class="c1"># 遊戲訓練的總episode數量</span>
<span class="n">num_exploration_episodes</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># 探索過程所占的episode數量</span>
<span class="n">max_len_episode</span> <span class="o">=</span> <span class="mi">1000</span>          <span class="c1"># 每個episode的最大回合數</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>                 <span class="c1"># 批次大小</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>            <span class="c1"># 學習率</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">1.</span>                      <span class="c1"># 折扣因子</span>
<span class="n">initial_epsilon</span> <span class="o">=</span> <span class="mf">1.</span>            <span class="c1"># 探索起始時的探索率</span>
<span class="n">final_epsilon</span> <span class="o">=</span> <span class="mf">0.01</span>            <span class="c1"># 探索終止時的探索率</span>
</pre></div>
</div>
<p>然後，我們使用 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 建立一個Q函數網路（Q-network），用於擬合Q Learning中的Q函數。這裡我們使用較簡單的多層全連接神經網路進行擬合。該網路輸入當前狀態，輸出各個動作下的Q-value（CartPole下為2維，即向左和向右推動小車）。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">QNetwork</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>最後，我們在主程式中實現Q Learning演算法。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">)</span>       <span class="c1"># 實例化一個遊戲環境，參數為遊戲名稱</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">QNetwork</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span> <span class="c1"># 使用一個 deque 作為 Q Learning 的經驗回放池</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">initial_epsilon</span>
    <span class="k">for</span> <span class="n">episode_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>             <span class="c1"># 初始化環境，獲得初始狀態</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>                  <span class="c1"># 計算當前探索率</span>
            <span class="n">initial_epsilon</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_exploration_episodes</span> <span class="o">-</span> <span class="n">episode_id</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_exploration_episodes</span><span class="p">,</span>
            <span class="n">final_epsilon</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len_episode</span><span class="p">):</span>
            <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>                                <span class="c1"># 對當前影像進行渲染，繪圖到影像</span>
            <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>               <span class="c1"># epsilon-greedy 探索策略，以 epsilon 的機率選擇隨機動作</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>      <span class="c1"># 選擇隨機動作（探索）</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>   <span class="c1"># 選擇模型計算出的 Q Value 最大的動作</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">action</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># 讓環境執行動作，獲得執行完動作的下一個狀態，動作的獎勵，遊戲是否已結束以及額外資訊</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="c1"># 如果遊戲Game Over，給予大的負獎勵</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mf">10.</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="n">reward</span>
            <span class="c1"># 將(state, action, reward, next_state)的四元組（外加 done 標籤表示是否結束）放入經驗回放池</span>
            <span class="n">replay_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="mi">0</span><span class="p">))</span>
            <span class="c1"># 更新當前 state</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>                                    <span class="c1"># 遊戲結束則退出本輪循環，進行下一個 episode</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;episode </span><span class="si">%d</span><span class="s2">, epsilon </span><span class="si">%f</span><span class="s2">, score </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">episode_id</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>
                <span class="k">break</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">batch_size</span><span class="p">:</span>
                <span class="c1"># 從經驗回放池中隨機取一個批次的四元組，並分別轉換為 NumPy 陣列</span>
                <span class="n">batch_state</span><span class="p">,</span> <span class="n">batch_action</span><span class="p">,</span> <span class="n">batch_reward</span><span class="p">,</span> <span class="n">batch_next_state</span><span class="p">,</span> <span class="n">batch_done</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
                    <span class="o">*</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">replay_buffer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">))</span>
                <span class="n">batch_state</span><span class="p">,</span> <span class="n">batch_reward</span><span class="p">,</span> <span class="n">batch_next_state</span><span class="p">,</span> <span class="n">batch_done</span> <span class="o">=</span> \
                    <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="p">[</span><span class="n">batch_state</span><span class="p">,</span> <span class="n">batch_reward</span><span class="p">,</span> <span class="n">batch_next_state</span><span class="p">,</span> <span class="n">batch_done</span><span class="p">]]</span>
                <span class="n">batch_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_action</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

                <span class="n">q_value</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_next_state</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">batch_reward</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">q_value</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">batch_done</span><span class="p">)</span>  <span class="c1"># 計算 y 值</span>
                <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span>  <span class="c1"># 最小化 y 和 Q-value 的距離</span>
                        <span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                        <span class="n">y_pred</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">batch_state</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">batch_action</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads_and_vars</span><span class="o">=</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">))</span>       <span class="c1"># 計算梯度並更新參數</span>
</pre></div>
</div>
<p>對於不同的任務（或者說環境），我們需要根據任務的特點，設計不同的狀態以及採取合適的網路來擬合 Q 函數。例如，如果我們考慮經典的打磚塊游戲（Gym 環境套件中的  <a class="reference external" href="https://gym.openai.com/envs/Breakout-v0/">Breakout-v0</a> ），每一次執行動作（擋板向左、向右或不動），都會返回一個 <code class="docutils literal notranslate"><span class="pre">210</span> <span class="pre">*</span> <span class="pre">160</span> <span class="pre">*</span> <span class="pre">3</span></code> 的 RGB 圖片，表示當前影像畫面。為了讓打磚塊遊戲這個任務設計合適的狀態表示，我們有以下分析：</p>
<ul class="simple">
<li><p>磚塊的顏色資訊並不是很重要，畫面轉換成灰階也不影響操作，因此可以去除狀態中的顏色資訊（即將圖片轉為灰階表示）；</p></li>
<li><p>小球移動的資訊很重要，如果只知道單一張影像畫面而不知道小球往哪邊移動，即使是人也很難判斷擋板應當移動的方向。因此，必須在狀態中加入小球運動方向的資訊。一個簡單的方式是將當前影像與前面幾張影像畫面進行疊加，得到一個 <code class="docutils literal notranslate"><span class="pre">210</span> <span class="pre">*</span> <span class="pre">160</span> <span class="pre">*</span> <span class="pre">X</span></code> （X 為疊加影格數）的狀態表示；</p></li>
<li><p>每個影格數的分辨率不需要特別高，只要能大致瞭解方塊、小球和擋板的位置以做出決策即可，因此對於每張影像的長寬可做適當壓縮。</p></li>
</ul>
<p>而考慮到我們需要從圖片資訊中提取特徵，使用 CNN 作為擬合 Q 函數的網路將更為適合。由此，將上面的 <code class="docutils literal notranslate"><span class="pre">QNetwork</span></code> 更換為 CNN 網路，並對狀態做一些修改，即可用於玩一些簡單的影像遊戲。</p>
</div>
<div class="section" id="keras-pipeline">
<h2>Keras Pipeline *<a class="headerlink" href="#keras-pipeline" title="永久链接至标题">¶</a></h2>
<p>以上範例均使用了 Keras 的 Subclassing API 建立模型，即對 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 類進行擴展以定義自己的新模型，同時人工編寫了訓練和評估模型的流程。這種方式靈活度高，且與其他流行的深度學習框架（如 PyTorch、Chainer）共通，是本手冊所推薦的方法。不過在很多時候，我們只需要建立一個結構相對簡單和典型的神經網路（比如上文中的 MLP 和 CNN），並使用常見的手法進行訓練。這時，Keras 也給我們提供了另一套更為簡單高效的內建方法來建立、訓練和評估模型。</p>
<div class="section" id="keras-sequential-functional-api">
<span id="sequential-functional"></span><h3>Keras Sequential/Functional API 模式建立模型<a class="headerlink" href="#keras-sequential-functional-api" title="永久链接至标题">¶</a></h3>
<p>最典型和常用的神經網路結構是將一堆層按特定順序疊加起來，那麼，我們是不是只需要提供一個層的列表，就能由 Keras 將它們自動首尾相連，形成模型呢？Keras 的 Sequential API 正是如此。通過向 <code class="docutils literal notranslate"><span class="pre">tf.keras.models.Sequential()</span></code> 提供一個層的列表，就能快速地建立一個 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 模型並返回：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>        <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
        <span class="p">])</span>
</pre></div>
</div>
<p>不過，這種層疊結構並不能表示任意的神經網路結構。為此，Keras 提供了 Functional API，幫助我們建立更為複雜的模型，例如多輸入 / 輸出或存在參數共用的模型。其使用方法是將層作為可調用的對象並返回張量（這點與之前章節的使用方法一致），並將輸入向量和輸出向量提供給 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 的 <code class="docutils literal notranslate"><span class="pre">inputs</span></code> 和 <code class="docutils literal notranslate"><span class="pre">outputs</span></code> 參數，範例如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="keras-model-compile-fit-evaluate">
<h3>使用 Keras Model 的 <code class="docutils literal notranslate"><span class="pre">compile</span></code> 、 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 和 <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> 方法訓練和評估模型<a class="headerlink" href="#keras-model-compile-fit-evaluate" title="永久链接至标题">¶</a></h3>
<p>當模型建立完成後，通過 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 的 <code class="docutils literal notranslate"><span class="pre">compile</span></code> 方法配置訓練過程：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">sparse_categorical_accuracy</span><span class="p">]</span>
    <span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tf.keras.Model.compile</span></code> 接受 3 個重要的參數：</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">oplimizer</span></code> ：優化器，可從 <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers</span></code> 中選擇；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss</span></code> ：損失函數，可從 <code class="docutils literal notranslate"><span class="pre">tf.keras.losses</span></code> 中選擇；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">metrics</span></code> ：評量指標，可從 <code class="docutils literal notranslate"><span class="pre">tf.keras.metrics</span></code> 中選擇。</p></li>
</ul>
</div></blockquote>
<p>接下來，可以使用 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 的 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 方法訓練模型：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_loader</span><span class="o">.</span><span class="n">train_data</span><span class="p">,</span> <span class="n">data_loader</span><span class="o">.</span><span class="n">train_label</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tf.keras.Model.fit</span></code> 接受 5 個重要的參數：</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x</span></code> ：訓練資料；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y</span></code> ：目標資料（資料標籤）；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">epochs</span></code> ：將訓練資料疊代多少遍；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code> ：批次的大小；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">validation_data</span></code> ：驗證資料，可用於在訓練過程中監控模型的性能。</p></li>
</ul>
</div></blockquote>
<p>Keras 支援使用 <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> 進行訓練，詳見 <a class="reference internal" href="tools.html#tfdata"><span class="std std-ref">tf.data</span></a> 。</p>
<p>最後，使用 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model.evaluate</span></code> 評估訓練效果，提供測試資料及標籤即可：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">data_loader</span><span class="o">.</span><span class="n">test_data</span><span class="p">,</span> <span class="n">data_loader</span><span class="o">.</span><span class="n">test_label</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id23">
<h2>自定義層、損失函數和評量指標 *<a class="headerlink" href="#id23" title="永久链接至标题">¶</a></h2>
<p>可能你還會問，如果現有的這些層無法滿足我的要求，我需要定義自己的層怎麼辦？事實上，我們不僅可以繼承 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 編寫自己的模型類，也可以繼承 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code> 編寫自己的層。</p>
<div class="section" id="custom-layer">
<span id="id24"></span><h3>自定義層<a class="headerlink" href="#custom-layer" title="永久链接至标题">¶</a></h3>
<p>自定義層需要繼承 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code> 類，並覆寫 <code class="docutils literal notranslate"><span class="pre">__init__</span></code> 、 <code class="docutils literal notranslate"><span class="pre">build</span></code> 和 <code class="docutils literal notranslate"><span class="pre">call</span></code> 三個方法，如下所示：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 初始化程式碼</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>     <span class="c1"># input_shape 是一個 TensorShape 類型對象，提供輸入的形狀</span>
        <span class="c1"># 在第一次使用該層的時候呼叫該部分程式碼，在這裡創建變數可以使得變數的形狀自適應輸入</span>
        <span class="c1"># 而不需要使用者額外指定變數形狀。</span>
        <span class="c1"># 如果已經可以完全確定變數的形狀，也可以在__init__部分創建變數</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">variable_0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">variable_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># 模型呼叫的程式碼（處理輸入並返回輸出）</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>例如，如果我們要自己實現一個 <a class="reference internal" href="#linear"><span class="std std-ref">本章第一節</span></a> 中的全連接層（ <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Dense</span></code> ），可以按以下方式編寫。此程式碼在 <code class="docutils literal notranslate"><span class="pre">build</span></code> 方法中創建兩個變數，並在 <code class="docutils literal notranslate"><span class="pre">call</span></code> 方法中使用創建的變數進行運算：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LinearLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">units</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>     <span class="c1"># 這裡 input_shape 是第一次運行call()時參數inputs的形狀</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        <span class="k">return</span> <span class="n">y_pred</span>
</pre></div>
</div>
<p>在定義模型的時候，我們便可以如同 Keras 中的其他層一樣，呼叫我們自定義的層 <code class="docutils literal notranslate"><span class="pre">LinearLayer</span></code>：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LinearModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">LinearLayer</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
<div class="section" id="id25">
<h3>自定義損失函數和評量指標<a class="headerlink" href="#id25" title="永久链接至标题">¶</a></h3>
<p>自定義損失函數需要繼承 <code class="docutils literal notranslate"><span class="pre">tf.keras.losses.Loss</span></code> 類別，重寫 <code class="docutils literal notranslate"><span class="pre">call</span></code> 方法即可，輸入真實值 <code class="docutils literal notranslate"><span class="pre">y_true</span></code> 和模型預測值 <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> ，輸出模型預測值和真實值之間通過自定義的損失函數計算出的損失值。下面的範例為均方差(Mean-Square Error, MSE）損失函數：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MeanSquaredError</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Loss</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">))</span>
</pre></div>
</div>
<p>自定義評量指標需要繼承 <code class="docutils literal notranslate"><span class="pre">tf.keras.metrics.Metric</span></code> 類，並重寫 <code class="docutils literal notranslate"><span class="pre">__init__</span></code> 、 <code class="docutils literal notranslate"><span class="pre">update_state</span></code> 和 <code class="docutils literal notranslate"><span class="pre">result</span></code> 三個方法。下面的範例對前面用到的 <code class="docutils literal notranslate"><span class="pre">SparseCategoricalAccuracy</span></code> 評量指標類別做了一個簡單的重實現：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SparseCategoricalAccuracy</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Metric</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;total&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;count&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">update_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y_true</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">values</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">result</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total</span>
</pre></div>
</div>
<dl class="citation">
<dt class="label" id="lecun1998"><span class="brackets"><a class="fn-backref" href="#id7">LeCun1998</a></span></dt>
<dd><ol class="upperalpha simple" start="25">
<li><p>LeCun, L. Bottou, Y. Bengio, and P. Haffner. “Gradient-based learning applied to document recognition.” Proceedings of the IEEE, 86(11):2278-2324, November 1998. <a class="reference external" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></p></li>
</ol>
</dd>
<dt class="label" id="graves2013"><span class="brackets"><a class="fn-backref" href="#id17">Graves2013</a></span></dt>
<dd><p>Graves, Alex. “Generating Sequences With Recurrent Neural Networks.” ArXiv:1308.0850 [Cs], August 4, 2013. http://arxiv.org/abs/1308.0850.</p>
</dd>
<dt class="label" id="mnih2013"><span class="brackets"><a class="fn-backref" href="#id22">Mnih2013</a></span></dt>
<dd><p>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. “Playing Atari with Deep Reinforcement Learning.” ArXiv:1312.5602 [Cs], December 19, 2013. http://arxiv.org/abs/1312.5602.</p>
</dd>
</dl>
<script>
    $(document).ready(function(){
        $(".rst-footer-buttons").after("<div id='discourse-comments'></div>");
        DiscourseEmbed = { discourseUrl: 'https://discuss.tf.wiki/', topicId: 190 };
        (function() {
            var d = document.createElement('script'); d.type = 'text/javascript'; d.async = true;
            d.src = DiscourseEmbed.discourseUrl + 'javascripts/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(d);
        })();
    });
</script></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tools.html" class="btn btn-neutral float-right" title="TensorFlow常用模組" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="basic.html" class="btn btn-neutral float-left" title="TensorFlow 基礎" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> 上一页</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2021, Xihan Li (snowkylin)

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.

  <p><a href="https://beian.miit.gov.cn/" target="_blank">沪ICP备13038357号-18</a ></p> 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-40509304-12', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>