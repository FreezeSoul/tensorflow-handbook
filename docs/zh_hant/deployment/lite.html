

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>TensorFlow Lite（Jinpeng） &mdash; 简单粗暴 TensorFlow 2 0.4 beta 文档</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/js/tw_cn.js"></script>
        <script src="../../_static/js/pangu.min.js"></script>
        <script src="../../_static/js/custom_20200921.js"></script>
        <script src="../../_static/translations.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="TensorFlow in JavaScript（Huan）" href="javascript.html" />
    <link rel="prev" title="TensorFlow Serving" href="serving.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> 简单粗暴 TensorFlow 2
          

          
          </a>

          
            
            
              <div class="version">
                0.4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">目录</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/foreword.html">推荐序</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/preface.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/introduction.html">TensorFlow概述</a></li>
</ul>
<p class="caption"><span class="caption-text">基础</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/installation.html">TensorFlow安装与环境配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/basic.html">TensorFlow基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/models.html">TensorFlow 模型建立与训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/tools.html">TensorFlow常用模块</a></li>
</ul>
<p class="caption"><span class="caption-text">部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/export.html">TensorFlow模型导出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/serving.html">TensorFlow Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/lite.html">TensorFlow Lite（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/javascript.html">TensorFlow in JavaScript（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">大规模训练与加速</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/distributed.html">TensorFlow分布式训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/tpu.html">使用TPU训练TensorFlow模型（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">扩展</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/tfhub.html">TensorFlow Hub 模型复用（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/tfds.html">TensorFlow Datasets 数据集载入</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/swift.html">Swift for TensorFlow (S4TF) (Huan）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/quantum.html">TensorFlow Quantum: 混合量子-经典机器学习 *</a></li>
</ul>
<p class="caption"><span class="caption-text">附录</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/rl.html">强化学习简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/docker.html">使用Docker部署TensorFlow环境</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/cloud.html">在云端使用TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/jupyterlab.html">部署自己的交互式Python开发环境JupyterLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/recommended_books.html">参考资料与推荐阅读</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/terms.html">术语中英对照表</a></li>
</ul>
<p class="caption"><span class="caption-text">目錄</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../preface.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">TensorFlow概述</a></li>
</ul>
<p class="caption"><span class="caption-text">基礎</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../basic/installation.html">TensorFlow 安裝與環境配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/basic.html">TensorFlow 基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/models.html">TensorFlow 模型建立與訓練</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/tools.html">TensorFlow常用模組</a></li>
</ul>
<p class="caption"><span class="caption-text">部署</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="export.html">TensorFlow模型匯出</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving.html">TensorFlow Serving</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">TensorFlow Lite（Jinpeng）</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">模型轉換</a></li>
<li class="toctree-l2"><a class="reference internal" href="#android">Android部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quantization">Quantization 模型轉換</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">總結</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="javascript.html">TensorFlow in JavaScript（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">大規模訓練與加速</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix/distributed.html">TensorFlow分布式訓練</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/tpu.html">使用TPU訓練TensorFlow模型（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">擴展</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix/tfhub.html">TensorFlow Hub 模型複用（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/tfds.html">TensorFlow Datasets 資料集載入</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/swift.html">Swift for TensorFlow (S4TF) (Huan）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/quantum.html">TensorFlow Quantum: 混合量子-經典機器學習 *</a></li>
</ul>
<p class="caption"><span class="caption-text">附錄</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix/rl.html">強化學習簡介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/docker.html">使用Docker部署TensorFlow環境</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/cloud.html">在雲端使用TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/jupyterlab.html">部署自己的互動式 Python 開發環境 JupyterLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/recommended_books.html">參考資料與推薦閱讀</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/terms.html">專有名詞中英對照表</a></li>
</ul>
<p class="caption"><span class="caption-text">Preface</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/introduction.html">TensorFlow Overview</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/installation.html">Installation and Environment Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/basic.html">TensorFlow Basic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/models.html">Model Construction and Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/tools.html">Common Modules in TensorFlow</a></li>
</ul>
<p class="caption"><span class="caption-text">Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/deployment/export.html">TensorFlow Model Export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/deployment/serving.html">TensorFlow Serving</a></li>
</ul>
<p class="caption"><span class="caption-text">Large-scale Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/appendix/distributed.html">Distributed training with TensorFlow</a></li>
</ul>
<p class="caption"><span class="caption-text">Extensions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/appendix/tfds.html">TensorFlow Datasets: Ready-to-use Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/appendix/quantum.html">TensorFlow Quantum: Hybrid Quantum-classical Machine Learning *</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">简单粗暴 TensorFlow 2</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>TensorFlow Lite（Jinpeng）</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/zh_hant/deployment/lite.rst.txt" rel="nofollow"> 查看页面源码</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tensorflow-lite-jinpeng">
<h1>TensorFlow Lite（Jinpeng）<a class="headerlink" href="#tensorflow-lite-jinpeng" title="永久链接至标题">¶</a></h1>
<p>TensorFlow Lite 是 TensorFlow 在可攜式和 IoT 等邊緣設備端的解決方案，提供了 Java、Python 和 C++ API 庫，可以執行在 Android、iOS 和 Raspberry Pi 等設備上。2019 年是 5G 元年，萬物互聯的時代已經來臨，作為 TensorFlow 在邊緣設備上的基礎設施，TFLite 將會是越來越重要的角色。</p>
<p>目前 TFLite 只提供了推論功能，在伺服器端進行訓練後，經過如下簡單處理即可部署到邊緣設備上。</p>
<ul class="simple">
<li><p>模型轉換：由於邊緣設備計算等資源有限，使用 TensorFlow 訓練好的模型，模型太大、執行效率比較低，不能直接在可攜式裝置部署，需要透過相應工具進行轉換成適合邊緣設備的格式。</p></li>
<li><p>邊緣設備部署：本章節以 Android 為例，簡單介紹如何在 Android 應用中部署轉化後的模型，完成 Mnist 圖片的辨識。</p></li>
</ul>
<div class="section" id="id1">
<h2>模型轉換<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h2>
<p>轉換工具有兩種：終端機工具和 Python API</p>
<p>TF2.0 對模型轉換工具發生了非常大的變化，推薦大家使用 Python API 進行轉換，終端機工具只提供了基本的轉化功能。轉換後的原模型為 <code class="docutils literal notranslate"><span class="pre">FlatBuffers</span></code> 格式。 <code class="docutils literal notranslate"><span class="pre">FlatBuffers</span></code> 原來主要應用於遊戲場景，是Google為了高性能場景創建的序列化函式庫，相比 Protocol Buffer 有更高的性能和更小的檔案等優勢，更適合於邊緣設備部署。</p>
<p>轉換方式有兩種：Float 格式和 Quantized 格式</p>
<p>為了熟悉兩種方式我們都會使用，針對 Float 格式的，先使用終端機工具 <code class="docutils literal notranslate"><span class="pre">tflite_convert</span></code> ，跟著 TensorFlow 一起安裝（見 <a class="reference external" href="https://tf.wiki/zh_hant/basic/installation.html#tensorflow">一般安裝步驟</a> ）。</p>
<p>在終端機執行下列指令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tflite_convert</span> <span class="o">-</span><span class="n">h</span>
</pre></div>
</div>
<p>輸出結果如下，該指令的使用方法：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">tflite_convert</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="o">--</span><span class="n">output_file</span> <span class="n">OUTPUT_FILE</span>
                      <span class="p">(</span><span class="o">--</span><span class="n">saved_model_dir</span> <span class="n">SAVED_MODEL_DIR</span> <span class="o">|</span> <span class="o">--</span><span class="n">keras_model_file</span> <span class="n">KERAS_MODEL_FILE</span><span class="p">)</span>
  <span class="o">--</span><span class="n">output_file</span> <span class="n">OUTPUT_FILE</span>
                        <span class="n">Full</span> <span class="n">filepath</span> <span class="n">of</span> <span class="n">the</span> <span class="n">output</span> <span class="n">file</span><span class="o">.</span>
  <span class="o">--</span><span class="n">saved_model_dir</span> <span class="n">SAVED_MODEL_DIR</span>
                        <span class="n">Full</span> <span class="n">path</span> <span class="n">of</span> <span class="n">the</span> <span class="n">directory</span> <span class="n">containing</span> <span class="n">the</span> <span class="n">SavedModel</span><span class="o">.</span>
  <span class="o">--</span><span class="n">keras_model_file</span> <span class="n">KERAS_MODEL_FILE</span>
                        <span class="n">Full</span> <span class="n">filepath</span> <span class="n">of</span> <span class="n">HDF5</span> <span class="n">file</span> <span class="n">containing</span> <span class="n">tf</span><span class="o">.</span><span class="n">Keras</span> <span class="n">model</span><span class="o">.</span>
</pre></div>
</div>
<p>在 <a class="reference external" href="https://tf.wiki/zh_hant/deployment/export.html#tensorflow">TensorFlow 模型匯出</a> 中，我們知道 TF2.0 支援兩種模型匯出方法和格式 SavedModel 和 Keras Sequential。</p>
<p>SavedModel 匯出模型轉換：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tflite_convert --saved_model_dir<span class="o">=</span>saved/1 --output_file<span class="o">=</span>mnist_savedmodel.tflite
</pre></div>
</div>
<p>Keras Sequential 匯出模型轉換：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tflite_convert --keras_model_file<span class="o">=</span>mnist_cnn.h5 --output_file<span class="o">=</span>mnist_sequential.tflite
</pre></div>
</div>
<p>到此，已經得到兩個 TensorFlow Lite 模型。因為兩者後續操作基本一致，我們只處理 SavedModel 格式的，Keras Sequential 的轉換可以按照相同的方法處理。</p>
</div>
<div class="section" id="android">
<h2>Android部署<a class="headerlink" href="#android" title="永久链接至标题">¶</a></h2>
<p>現在開始在 Android 環境部署，需要先給 Android Studio 配置 proxy 的鏡像網址。</p>
<p><strong>配置build.gradle</strong></p>
<p>將 <code class="docutils literal notranslate"><span class="pre">build.gradle</span></code> 中的 maven 來源 <code class="docutils literal notranslate"><span class="pre">google()</span></code> 和 <code class="docutils literal notranslate"><span class="pre">jcenter()</span></code> 分別替換為鏡像網址，如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">buildscript</span> <span class="p">{</span>

    <span class="n">repositories</span> <span class="p">{</span>
        <span class="n">maven</span> <span class="p">{</span> <span class="n">url</span> <span class="s1">&#39;https://maven.aliyun.com/nexus/content/repositories/google&#39;</span> <span class="p">}</span>
        <span class="n">maven</span> <span class="p">{</span> <span class="n">url</span> <span class="s1">&#39;https://maven.aliyun.com/nexus/content/repositories/jcenter&#39;</span> <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">dependencies</span> <span class="p">{</span>
        <span class="n">classpath</span> <span class="s1">&#39;com.android.tools.build:gradle:3.5.1&#39;</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="n">allprojects</span> <span class="p">{</span>
    <span class="n">repositories</span> <span class="p">{</span>
        <span class="n">maven</span> <span class="p">{</span> <span class="n">url</span> <span class="s1">&#39;https://maven.aliyun.com/nexus/content/repositories/google&#39;</span> <span class="p">}</span>
        <span class="n">maven</span> <span class="p">{</span> <span class="n">url</span> <span class="s1">&#39;https://maven.aliyun.com/nexus/content/repositories/jcenter&#39;</span> <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>配置app/build.gradle</strong></p>
<p>新建一個 Android Project，打開 <code class="docutils literal notranslate"><span class="pre">app/build.gradle</span></code> 添加如下資訊：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>android {
    aaptOptions {
        noCompress &quot;tflite&quot; // 編譯apk時，不壓縮tflite文件
    }
}

dependencies {
    implementation &#39;org.tensorflow:tensorflow-lite:1.14.0&#39;
}
</pre></div>
</div>
<p>其中，</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">aaptOptions</span></code> 設置 tflite 文件不壓縮，確保後面 tflite 文件可以被 Interpreter 正確載入。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.tensorflow:tensorflow-lite</span></code> 的最新版本號碼可以在這裡查詢 <a class="reference external" href="https://bintray.com/google/tensorflow/tensorflow-lite">https://bintray.com/google/tensorflow/tensorflow-lite</a></p></li>
</ol>
<p>設置好之後，sync 和 build 整個程式包，如果 build 成功說明，配置成功。</p>
<p><strong>添加 tflite 文件到 assets 資料夾</strong></p>
<p>在 app 目錄先新建 assets 目錄，並將 <code class="docutils literal notranslate"><span class="pre">mnist_savedmodel.tflite</span></code> 文件保存到assets目錄。重新編譯apk，檢查新編譯出來的 apk 的 assets 資料夾是否有 <code class="docutils literal notranslate"><span class="pre">mnist_cnn.tflite</span></code> 文件。</p>
<p>點擊選單 Build-&gt;Build APK(s) 觸發 apk 編譯，apk 編譯成功點擊右下角的 EventLog。點擊最後一條資訊中的 <code class="docutils literal notranslate"><span class="pre">analyze</span></code> 連結，會觸發 apk analyzer 查看新編譯出來的 apk，若在 assets 目錄下存在 <code class="docutils literal notranslate"><span class="pre">mnist_savedmodel.tflite</span></code> ，則編譯打包成功，如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">assets</span>
     <span class="o">|</span><span class="n">__mnist_savedmodel</span><span class="o">.</span><span class="n">tflite</span>
</pre></div>
</div>
<p><strong>載入模型</strong></p>
<p>使用如下指令將 <code class="docutils literal notranslate"><span class="pre">mnist_savedmodel.tflite</span></code> 文件載入到 memory-map 中，作為 Interpreter 實例化的輸入</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="cm">/** Memory-map the model file in Assets. */</span>
<span class="kd">private</span> <span class="n">MappedByteBuffer</span> <span class="nf">loadModelFile</span><span class="p">(</span><span class="n">Activity</span> <span class="n">activity</span><span class="p">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="p">{</span>
    <span class="n">AssetFileDescriptor</span> <span class="n">fileDescriptor</span> <span class="o">=</span> <span class="n">activity</span><span class="p">.</span><span class="na">getAssets</span><span class="p">().</span><span class="na">openFd</span><span class="p">(</span><span class="n">mModelPath</span><span class="p">);</span>
    <span class="n">FileInputStream</span> <span class="n">inputStream</span> <span class="o">=</span> <span class="k">new</span> <span class="n">FileInputStream</span><span class="p">(</span><span class="n">fileDescriptor</span><span class="p">.</span><span class="na">getFileDescriptor</span><span class="p">());</span>
    <span class="n">FileChannel</span> <span class="n">fileChannel</span> <span class="o">=</span> <span class="n">inputStream</span><span class="p">.</span><span class="na">getChannel</span><span class="p">();</span>
    <span class="kt">long</span> <span class="n">startOffset</span> <span class="o">=</span> <span class="n">fileDescriptor</span><span class="p">.</span><span class="na">getStartOffset</span><span class="p">();</span>
    <span class="kt">long</span> <span class="n">declaredLength</span> <span class="o">=</span> <span class="n">fileDescriptor</span><span class="p">.</span><span class="na">getDeclaredLength</span><span class="p">();</span>
    <span class="k">return</span> <span class="n">fileChannel</span><span class="p">.</span><span class="na">map</span><span class="p">(</span><span class="n">FileChannel</span><span class="p">.</span><span class="na">MapMode</span><span class="p">.</span><span class="na">READ_ONLY</span><span class="p">,</span> <span class="n">startOffset</span><span class="p">,</span> <span class="n">declaredLength</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p>memory-map 可以把整個文件映射到虛擬記憶體中，用於提升 tflite 模型的讀取性能。更多請參考： <a class="reference external" href="https://docs.oracle.com/javase/8/docs/api/java/nio/channels/FileChannel.html#map-java.nio.channels.FileChannel.MapMode-long-long-">JDK API介紹</a></p>
</div>
<p>實例化 Interpreter，其中 acitivity 是為了從 assets 中獲取模型，因為我們把模型編譯到 assets 中，只能透過 <code class="docutils literal notranslate"><span class="pre">getAssets()</span></code> 打開。</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">mTFLite</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Interpreter</span><span class="p">(</span><span class="n">loadModelFile</span><span class="p">(</span><span class="n">activity</span><span class="p">));</span>
</pre></div>
</div>
<p>memory-map後的 <code class="docutils literal notranslate"><span class="pre">MappedByteBuffer</span></code> 直接作為 <code class="docutils literal notranslate"><span class="pre">Interpreter</span></code> 的輸入， <code class="docutils literal notranslate"><span class="pre">mTFLite</span></code> （ <code class="docutils literal notranslate"><span class="pre">Interpreter</span></code> ）就是轉換後模型的執行載體。</p>
<p><strong>執行輸入</strong></p>
<p>我們使用 MNIST test 測試集中的圖片作為輸入，mnist 圖像大小 28*28，單像素，因為我們輸入的資料需要設置成如下格式</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="c1">//Float模型相關參數</span>
<span class="c1">// com/dpthinker/mnistclassifier/model/FloatSavedModelConfig.java</span>
<span class="kd">protected</span> <span class="kt">void</span> <span class="nf">setConfigs</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">setModelName</span><span class="p">(</span><span class="s">&quot;mnist_savedmodel.tflite&quot;</span><span class="p">);</span>

    <span class="n">setNumBytesPerChannel</span><span class="p">(</span><span class="mi">4</span><span class="p">);</span>

    <span class="n">setDimBatchSize</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="n">setDimPixelSize</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

    <span class="n">setDimImgWeight</span><span class="p">(</span><span class="mi">28</span><span class="p">);</span>
    <span class="n">setDimImgHeight</span><span class="p">(</span><span class="mi">28</span><span class="p">);</span>

    <span class="n">setImageMean</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
    <span class="n">setImageSTD</span><span class="p">(</span><span class="mf">255.0f</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// 初始化</span>
<span class="c1">// com/dpthinker/mnistclassifier/classifier/BaseClassifier.java</span>
<span class="kd">private</span> <span class="kt">void</span> <span class="nf">initConfig</span><span class="p">(</span><span class="n">BaseModelConfig</span> <span class="n">config</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">this</span><span class="p">.</span><span class="na">mModelConfig</span> <span class="o">=</span> <span class="n">config</span><span class="p">;</span>
    <span class="k">this</span><span class="p">.</span><span class="na">mNumBytesPerChannel</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="na">getNumBytesPerChannel</span><span class="p">();</span>
    <span class="k">this</span><span class="p">.</span><span class="na">mDimBatchSize</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="na">getDimBatchSize</span><span class="p">();</span>
    <span class="k">this</span><span class="p">.</span><span class="na">mDimPixelSize</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="na">getDimPixelSize</span><span class="p">();</span>
    <span class="k">this</span><span class="p">.</span><span class="na">mDimImgWidth</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="na">getDimImgWeight</span><span class="p">();</span>
    <span class="k">this</span><span class="p">.</span><span class="na">mDimImgHeight</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="na">getDimImgHeight</span><span class="p">();</span>
    <span class="k">this</span><span class="p">.</span><span class="na">mModelPath</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="na">getModelName</span><span class="p">();</span>
<span class="p">}</span>
</pre></div>
</div>
<p>將 MNIST 圖片轉化成 <code class="docutils literal notranslate"><span class="pre">ByteBuffer</span></code> ，並保持到 <code class="docutils literal notranslate"><span class="pre">imgData</span></code> （  <code class="docutils literal notranslate"><span class="pre">ByteBuffer</span></code> ）中</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="c1">// 將輸入的Bitmap轉化為Interpreter可以辨識的ByteBuffer</span>
<span class="c1">// com/dpthinker/mnistclassifier/classifier/BaseClassifier.java</span>
<span class="kd">protected</span> <span class="n">ByteBuffer</span> <span class="nf">convertBitmapToByteBuffer</span><span class="p">(</span><span class="n">Bitmap</span> <span class="n">bitmap</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span><span class="o">[]</span> <span class="n">intValues</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">int</span><span class="o">[</span><span class="n">mDimImgWidth</span> <span class="o">*</span> <span class="n">mDimImgHeight</span><span class="o">]</span><span class="p">;</span>
    <span class="n">scaleBitmap</span><span class="p">(</span><span class="n">bitmap</span><span class="p">).</span><span class="na">getPixels</span><span class="p">(</span><span class="n">intValues</span><span class="p">,</span>
            <span class="mi">0</span><span class="p">,</span> <span class="n">bitmap</span><span class="p">.</span><span class="na">getWidth</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">bitmap</span><span class="p">.</span><span class="na">getWidth</span><span class="p">(),</span> <span class="n">bitmap</span><span class="p">.</span><span class="na">getHeight</span><span class="p">());</span>

    <span class="n">ByteBuffer</span> <span class="n">imgData</span> <span class="o">=</span> <span class="n">ByteBuffer</span><span class="p">.</span><span class="na">allocateDirect</span><span class="p">(</span>
            <span class="n">mNumBytesPerChannel</span> <span class="o">*</span> <span class="n">mDimBatchSize</span> <span class="o">*</span> <span class="n">mDimImgWidth</span> <span class="o">*</span> <span class="n">mDimImgHeight</span> <span class="o">*</span> <span class="n">mDimPixelSize</span><span class="p">);</span>
    <span class="n">imgData</span><span class="p">.</span><span class="na">order</span><span class="p">(</span><span class="n">ByteOrder</span><span class="p">.</span><span class="na">nativeOrder</span><span class="p">());</span>
    <span class="n">imgData</span><span class="p">.</span><span class="na">rewind</span><span class="p">();</span>

    <span class="c1">// Convert the image toFloating point.</span>
    <span class="kt">int</span> <span class="n">pixel</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">mDimImgWidth</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">mDimImgHeight</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">//final int val = intValues[pixel++];</span>
            <span class="kt">int</span> <span class="n">val</span> <span class="o">=</span> <span class="n">intValues</span><span class="o">[</span><span class="n">pixel</span><span class="o">++]</span><span class="p">;</span>
            <span class="n">mModelConfig</span><span class="p">.</span><span class="na">addImgValue</span><span class="p">(</span><span class="n">imgData</span><span class="p">,</span> <span class="n">val</span><span class="p">);</span> <span class="c1">//添加把Pixel數值轉化並添加到ByteBuffer</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">imgData</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// mModelConfig.addImgValue定義</span>
<span class="c1">// com/dpthinker/mnistclassifier/model/FloatSavedModelConfig.java</span>
<span class="kd">public</span> <span class="kt">void</span> <span class="nf">addImgValue</span><span class="p">(</span><span class="n">ByteBuffer</span> <span class="n">imgData</span><span class="p">,</span> <span class="kt">int</span> <span class="n">val</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">imgData</span><span class="p">.</span><span class="na">putFloat</span><span class="p">(((</span><span class="n">val</span> <span class="o">&amp;</span> <span class="mh">0xFF</span><span class="p">)</span> <span class="o">-</span> <span class="n">getImageMean</span><span class="p">())</span> <span class="o">/</span> <span class="n">getImageSTD</span><span class="p">());</span>
<span class="p">}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">convertBitmapToByteBuffer</span></code> 的輸出即為模型執行的輸入。</p>
<p><strong>執行輸出</strong></p>
<p>定義一個 1*10 的多維陣列，因為我們只有 10 個 label，具體程式碼如下</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">privateFloat</span><span class="o">[][]</span> <span class="n">mLabelProbArray</span> <span class="o">=</span> <span class="n">newFloat</span><span class="o">[</span><span class="mi">1</span><span class="o">][</span><span class="mi">10</span><span class="o">]</span><span class="p">;</span>
</pre></div>
</div>
<p>執行結束後，每個二級元素都是一個label的機率。</p>
<p><strong>執行及結果處理</strong></p>
<p>開始執行模型，具體程式碼如下</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">mTFLite</span><span class="p">.</span><span class="na">run</span><span class="p">(</span><span class="n">imgData</span><span class="p">,</span> <span class="n">mLabelProbArray</span><span class="p">);</span>
</pre></div>
</div>
<p>針對某個圖片，執行後 <code class="docutils literal notranslate"><span class="pre">mLabelProbArray</span></code> 的內容就是各個 label 辨識的機率。對他們進行排序，找出機率最高的 label 並顯示辨識結果給用戶.</p>
<p>在Android應用中，筆者使用了 <code class="docutils literal notranslate"><span class="pre">View.OnClickListener()</span></code> 觸發 <code class="docutils literal notranslate"><span class="pre">&quot;image/*&quot;</span></code> 類型的 <code class="docutils literal notranslate"><span class="pre">Intent.ACTION_GET_CONTENT</span></code> ，進而獲取設備上的圖片（只支援 MNIST 標準圖片）。然後，透過 <code class="docutils literal notranslate"><span class="pre">RadioButtion</span></code> 的選擇情況，確認載入哪種轉換後的模型，並觸發真正分類操作。這部分比較簡單，請讀者自行閱讀程式碼即可，不再重複介紹。</p>
<p>選取一張 MNIST 測試集中的圖片進行測試，得到結果如下：</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/mnist_float.png"><img alt="../../_images/mnist_float.png" src="../../_images/mnist_float.png" style="width: 40%;" /></a>
</div>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p>注意我們這裡直接用 <code class="docutils literal notranslate"><span class="pre">mLabelProbArray</span></code> 數值中的 index作為label了，因為 MNIST 的 label 完全跟 index 從 0 到 9 匹配。如果是其他的分類問題，需要根據實際情況進行轉換。</p>
</div>
</div>
<div class="section" id="quantization">
<h2>Quantization 模型轉換<a class="headerlink" href="#quantization" title="永久链接至标题">¶</a></h2>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p>Quantized 模型是對原模型進行轉換過程中，將 float 參數轉化為 uint8 類型，進而產生的模型會更小、執行更快，但是解析度會有所下降。</p>
</div>
<p>前面我們介紹了 Float 模型的轉換方法，接下來我們要展示 Quantized 模型，在 TF1.0 上，可以使用終端機工具轉換 Quantized 模型。在筆者嘗試的情況看在 TF2.0 上，終端機工具目前只能轉換為 Float 模型，Python API 只能轉換為 Quantized 模型。</p>
<p>Python API 轉換方法如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>import tensorflow as tf

<span class="nv">converter</span> <span class="o">=</span> tf.lite.TFLiteConverter.from_saved_model<span class="o">(</span><span class="s1">&#39;saved/1&#39;</span><span class="o">)</span>
converter.optimizations <span class="o">=</span> <span class="o">[</span>tf.lite.Optimize.DEFAULT<span class="o">]</span>
<span class="nv">tflite_quant_model</span> <span class="o">=</span> converter.convert<span class="o">()</span>
open<span class="o">(</span><span class="s2">&quot;mnist_savedmodel_quantized.tflite&quot;</span>, <span class="s2">&quot;wb&quot;</span><span class="o">)</span>.write<span class="o">(</span>tflite_quant_model<span class="o">)</span>
</pre></div>
</div>
<p>最終轉換後的 Quantized 模型即為同個目錄下的 <code class="docutils literal notranslate"><span class="pre">mnist_savedmodel_quantized.tflite</span></code> 。</p>
<p>相對 TF1.0，上面的方法簡化了很多，不需要考慮各種各樣的參數，谷歌一直在優化開發者的使用體驗。</p>
<p>在TF1.0上，我們可以使用 <code class="docutils literal notranslate"><span class="pre">tflite_convert</span></code> 獲得模型具體結構，然後通過 graphviz 轉換為 pdf 或 png 等方便查看。
在TF2.0上，提供了新的一步到位的工具 <code class="docutils literal notranslate"><span class="pre">visualize.py</span></code> ，直接轉換為 html 文件，除了模型結構，還有更清晰的關鍵資訊。</p>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p><code class="docutils literal notranslate"><span class="pre">visualize.py</span></code> 目前應該還是開發階段，使用前需要先從 github 下載最新的 <code class="docutils literal notranslate"><span class="pre">TensorFlow</span></code> 和 <code class="docutils literal notranslate"><span class="pre">FlatBuffers</span></code> 原始碼，並且兩者要在同一目錄，因為 <code class="docutils literal notranslate"><span class="pre">visualize.py</span></code> 原始碼中是按照兩者在同一目錄寫的呼叫路徑。</p>
<p>下載 TensorFlow：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone git@github.com:tensorflow/tensorflow.git
</pre></div>
</div>
<p>下載 FlatBuffers：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone git@github.com:google/flatbuffers.git
</pre></div>
</div>
<p>編譯 FlatBuffers：（筆者使用的 Mac，其他平台請大家自行配置，應該不麻煩）</p>
<ol class="arabic simple">
<li><p>下載cmake：執行 <code class="docutils literal notranslate"><span class="pre">brew</span> <span class="pre">install</span> <span class="pre">cmake</span></code></p></li>
<li><p>設置編譯環境：在 <code class="docutils literal notranslate"><span class="pre">FlatBuffers</span></code> 的根目錄，執行 <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">-G</span> <span class="pre">&quot;Unix</span> <span class="pre">Makefiles&quot;</span> <span class="pre">-DCMAKE_BUILD_TYPE=Release</span></code></p></li>
<li><p>編譯：在 <code class="docutils literal notranslate"><span class="pre">FlatBuffers</span></code> 的根目錄，執行 <code class="docutils literal notranslate"><span class="pre">make</span></code></p></li>
</ol>
<p>編譯完成後，會在跟目錄生成 <code class="docutils literal notranslate"><span class="pre">flatc</span></code>，這個可執行文件是 <code class="docutils literal notranslate"><span class="pre">visualize.py</span></code> 執行所依賴的。</p>
</div>
<p><strong>visualize.py使用方法</strong></p>
<p>在tensorflow/tensorflow/lite/tools 目錄下，執行以下指令</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python visualize.py mnist_savedmodel_quantized.tflite mnist_savedmodel_quantized.html
</pre></div>
</div>
<p>生成關鍵資訊的可視化圖表</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/visualize1.png"><img alt="../../_images/visualize1.png" src="../../_images/visualize1.png" style="width: 100%;" /></a>
</div>
<p>模型結構</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/visualize2.png"><img alt="../../_images/visualize2.png" src="../../_images/visualize2.png" style="width: 40%;" /></a>
</div>
<p>可以發現，Input/Output 格式都是 <code class="docutils literal notranslate"><span class="pre">FLOAT32</span></code> 的多維陣列，Input 的 min 和 max 分別是 0.0 和 255.0。</p>
<p>跟 Float 模型對比，Input/Output 格式是一致的，所以可以重複使用 Float 模型 Android 部署過程中的配置。</p>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p>暫不確定這裡是否是 TF2.0 上的優化，如果是這樣的話，對開發者來說是非常友好的，這樣就正規化 Float 和 Quantized 模型處理了。</p>
</div>
<p>具體配置如下：</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="c1">// Quantized模型相關參數</span>
<span class="c1">// com/dpthinker/mnistclassifier/model/QuantSavedModelConfig.java</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">QuantSavedModelConfig</span> <span class="kd">extends</span> <span class="n">BaseModelConfig</span> <span class="p">{</span>
    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">setConfigs</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">setModelName</span><span class="p">(</span><span class="s">&quot;mnist_savedmodel_quantized.tflite&quot;</span><span class="p">);</span>

        <span class="n">setNumBytesPerChannel</span><span class="p">(</span><span class="mi">4</span><span class="p">);</span>

        <span class="n">setDimBatchSize</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
        <span class="n">setDimPixelSize</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

        <span class="n">setDimImgWeight</span><span class="p">(</span><span class="mi">28</span><span class="p">);</span>
        <span class="n">setDimImgHeight</span><span class="p">(</span><span class="mi">28</span><span class="p">);</span>

        <span class="n">setImageMean</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
        <span class="n">setImageSTD</span><span class="p">(</span><span class="mf">255.0f</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">addImgValue</span><span class="p">(</span><span class="n">ByteBuffer</span> <span class="n">imgData</span><span class="p">,</span> <span class="kt">int</span> <span class="n">val</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">imgData</span><span class="p">.</span><span class="na">putFloat</span><span class="p">(((</span><span class="n">val</span> <span class="o">&amp;</span> <span class="mh">0xFF</span><span class="p">)</span> <span class="o">-</span> <span class="n">getImageMean</span><span class="p">())</span> <span class="o">/</span> <span class="n">getImageSTD</span><span class="p">());</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>執行結果如下:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/quantized.png"><img alt="../../_images/quantized.png" src="../../_images/quantized.png" style="width: 40%;" /></a>
</div>
<p>Float 模型與 Quantized 模型大小與性能對比：</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>模型類別</p></th>
<th class="head"><p>Float</p></th>
<th class="head"><p>Quantized</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>模型大小</p></td>
<td><p>312K</p></td>
<td><p>82K</p></td>
</tr>
<tr class="row-odd"><td><p>運行性能</p></td>
<td><p>5.858854ms</p></td>
<td><p>1.439062ms</p></td>
</tr>
</tbody>
</table>
<p>可以發現， Quantized 模型在模型大小和執行性能上相對 Float 模型都有非常大的提升。不過，在筆者測試的過程中，發現有些圖片在 Float 模型上辨識正確的，在 Quantized 模型上會辨識錯，可見 <code class="docutils literal notranslate"><span class="pre">Quantization</span></code> 對模型的辨識解析度還是有影響的。由於在邊緣設備上資源有限，因此需要在模型大小、執行速度與辨識解析度上找到平衡。</p>
</div>
<div class="section" id="id3">
<h2>總結<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<p>本節介紹如何從零開始部署 TFLite 到 Android 應用中，包括：</p>
<ol class="arabic simple">
<li><p>如何將訓練好的 MNIST SavedModel 模型，轉換為 Float 模型和 Quantized 模型</p></li>
<li><p>如何使用 <code class="docutils literal notranslate"><span class="pre">visualize.py</span></code> 和解讀其結果資訊</p></li>
<li><p>如何將轉換後的模型部署到 Android 應用中</p></li>
</ol>
<p>筆者剛開始寫這部分內容的時候還是 TF1.0，在最近（2019年10月初）跟TF2.0的時候，發現有了很多變化，整體上是比原來更簡單了。不過文件部分很多還是講的比較模糊，很多地方還是需要看原始碼摸索。</p>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p>本節Android相關程式碼存放路徑：
<code class="docutils literal notranslate"><span class="pre">https://github.com/snowkylin/tensorflow-handbook/tree/master/source/android</span></code></p>
</div>
<script>
    $(document).ready(function(){
        $(".rst-footer-buttons").after("<div id='discourse-comments'></div>");
        DiscourseEmbed = { discourseUrl: 'https://discuss.tf.wiki/', topicId: 194 };
        (function() {
            var d = document.createElement('script'); d.type = 'text/javascript'; d.async = true;
            d.src = DiscourseEmbed.discourseUrl + 'javascripts/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(d);
        })();
    });
</script></div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="javascript.html" class="btn btn-neutral float-right" title="TensorFlow in JavaScript（Huan）" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="serving.html" class="btn btn-neutral float-left" title="TensorFlow Serving" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> 上一页</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2021, Xihan Li (snowkylin)

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.

  <p><a href="https://beian.miit.gov.cn/" target="_blank">沪ICP备13038357号-18</a ></p> 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-40509304-12', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>