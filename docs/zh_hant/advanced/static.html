

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>圖執行模式下的 TensorFlow 2 &mdash; 简单粗暴 TensorFlow 2 0.4 beta 文档</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../_static/js/tw_cn.js"></script>
        <script type="text/javascript" src="../../_static/js/pangu.min.js"></script>
        <script type="text/javascript" src="../../_static/js/custom_20200921.js"></script>
        <script type="text/javascript" src="../../_static/translations.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> 简单粗暴 TensorFlow 2
          

          
          </a>

          
            
            
              <div class="version">
                0.4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">目录</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/foreword.html">推荐序</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/preface.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/introduction.html">TensorFlow概述</a></li>
</ul>
<p class="caption"><span class="caption-text">基础</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/installation.html">TensorFlow安装与环境配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/basic.html">TensorFlow基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/models.html">TensorFlow 模型建立与训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/tools.html">TensorFlow常用模块</a></li>
</ul>
<p class="caption"><span class="caption-text">部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/export.html">TensorFlow模型导出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/serving.html">TensorFlow Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/lite.html">TensorFlow Lite（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/javascript.html">TensorFlow in JavaScript（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">大规模训练与加速</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/distributed.html">TensorFlow分布式训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/tpu.html">使用TPU训练TensorFlow模型（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">扩展</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/tfhub.html">TensorFlow Hub 模型复用（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/tfds.html">TensorFlow Datasets 数据集载入</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/swift.html">Swift for TensorFlow (S4TF) (Huan）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/quantum.html">TensorFlow Quantum: 混合量子-经典机器学习 *</a></li>
</ul>
<p class="caption"><span class="caption-text">附录</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/rl.html">强化学习简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/docker.html">使用Docker部署TensorFlow环境</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/cloud.html">在云端使用TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/jupyterlab.html">部署自己的交互式Python开发环境JupyterLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/recommended_books.html">参考资料与推荐阅读</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/terms.html">术语中英对照表</a></li>
</ul>
<p class="caption"><span class="caption-text">目錄</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../preface.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">TensorFlow概述</a></li>
</ul>
<p class="caption"><span class="caption-text">基礎</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../basic/installation.html">TensorFlow 安裝與環境配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/basic.html">TensorFlow 基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/models.html">TensorFlow 模型建立與訓練</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/tools.html">TensorFlow常用模組</a></li>
</ul>
<p class="caption"><span class="caption-text">部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deployment/export.html">TensorFlow模型匯出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/serving.html">TensorFlow Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/lite.html">TensorFlow Lite（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/javascript.html">TensorFlow in JavaScript（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">大規模訓練與加速</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix/distributed.html">TensorFlow分布式訓練</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/tpu.html">使用TPU訓練TensorFlow模型（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">擴展</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix/tfhub.html">TensorFlow Hub 模型複用（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/tfds.html">TensorFlow Datasets 資料集載入</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/swift.html">Swift for TensorFlow (S4TF) (Huan）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/quantum.html">TensorFlow Quantum: 混合量子-經典機器學習 *</a></li>
</ul>
<p class="caption"><span class="caption-text">附錄</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix/rl.html">強化學習簡介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/docker.html">使用Docker部署TensorFlow環境</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/cloud.html">在雲端使用TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/jupyterlab.html">部署自己的互動式 Python 開發環境 JupyterLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/recommended_books.html">參考資料與推薦閱讀</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/terms.html">專有名詞中英對照表</a></li>
</ul>
<p class="caption"><span class="caption-text">Preface</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/introduction.html">TensorFlow Overview</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/installation.html">Installation and Environment Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/basic.html">TensorFlow Basic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/models.html">Model Construction and Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/tools.html">Common Modules in TensorFlow</a></li>
</ul>
<p class="caption"><span class="caption-text">Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/deployment/export.html">TensorFlow Model Export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/deployment/serving.html">TensorFlow Serving</a></li>
</ul>
<p class="caption"><span class="caption-text">Large-scale Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/appendix/distributed.html">Distributed training with TensorFlow</a></li>
</ul>
<p class="caption"><span class="caption-text">Extensions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/appendix/tfds.html">TensorFlow Datasets: Ready-to-use Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/appendix/quantum.html">TensorFlow Quantum: Hybrid Quantum-classical Machine Learning *</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">简单粗暴 TensorFlow 2</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>圖執行模式下的 TensorFlow 2</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/zh_hant/advanced/static.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tensorflow-2">
<h1>圖執行模式下的 TensorFlow 2<a class="headerlink" href="#tensorflow-2" title="永久链接至标题">¶</a></h1>
<p>儘管 TensorFlow 2 建議以即時執行模式（Eager Execution）作為主要執行模式，然而，圖執行模式（Graph Execution）作為 TensorFlow 2 之前的主要執行模式，依舊對於我們理解 TensorFlow 具有重要意義。尤其是當我們需要使用 <span class="xref std std-ref">tf.function</span> 時，對圖執行模式的理解更是不可或缺。</p>
<p>圖執行模式在 TensorFlow 1.X 和 2.X 版本中的 API 不同：</p>
<ul class="simple">
<li><p>在 TensorFlow 1.X 中，圖執行模式主要通過「直接建構計算圖 + <code class="docutils literal notranslate"><span class="pre">tf.Session</span></code>」 進行操作；</p></li>
<li><p>在 TensorFlow 2 中，圖執行模式主要通過 <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> 進行操作。</p></li>
</ul>
<p>在本章，我們將在 <span class="xref std std-ref">tf.function：圖執行模式</span> 一節的基礎上，進一步對圖執行模式的這兩種 API 進行對比說明，以幫助已熟悉 TensorFlow 1.X 的用戶過渡到 TensorFlow 2。</p>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p>TensorFlow 2 依然支持 TensorFlow 1.X 的 API。為了在 TensorFlow 2 中使用 TensorFlow 1.X 的 API ，我們可以使用 <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">tensorflow.compat.v1</span> <span class="pre">as</span> <span class="pre">tf</span></code> 導入 TensorFlow，並通過 <code class="docutils literal notranslate"><span class="pre">tf.disable_eager_execution()</span></code> 禁用默認的即時執行模式。</p>
</div>
<div class="section" id="tensorflow-1-1">
<h2>TensorFlow 1+1<a class="headerlink" href="#tensorflow-1-1" title="永久链接至标题">¶</a></h2>
<p>TensorFlow 的圖執行模式是一個符號式的（基於計算圖的）計算框架。簡而言之，如果你需要進行一系列計算，則需要依次進行如下兩步：</p>
<ul class="simple">
<li><p>建立一個「計算圖」，這個圖描述了如何將輸入資料通過一系列計算而得到輸出；</p></li>
<li><p>建立一個會話，並在會話中與計算圖進行交互，即向計算圖傳入計算所需的資料，並從計算圖中獲取結果。</p></li>
</ul>
<div class="section" id="id1">
<h3>使用計算圖進行基本運算<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h3>
<p>這裡以計算 1+1 作為 Hello World 的示例。以下程式碼通過 TensorFlow 1.X 的圖執行模式 API 計算 1+1：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow.compat.v1</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">disable_eager_execution</span><span class="p">()</span>

<span class="c1"># 以下三行定義了一個簡單的“計算圖”</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 定義一個常數變數（Tensor）</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>           <span class="c1"># 等同於 c = tf.add(a, b)，c是變數a和變數b通過 tf.add 這一操作（Operation）所形成的新變數</span>
<span class="c1"># 到此為止，計算圖定義完畢，然而程式還沒有進行任何實質計算。</span>
<span class="c1"># 如果此時直接輸出變數 c 的值，是無法獲得 c = 2 的結果的</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>     <span class="c1"># 實例化一個會話（Session）</span>
<span class="n">c_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>        <span class="c1"># 通過會話的 run() 方法對計算圖里的節點（變數）進行實際的計算</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c_</span><span class="p">)</span>
</pre></div>
</div>
<p>輸出:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2</span>
</pre></div>
</div>
<p>而在 TensorFlow 2 中，我們將計算圖的建立步驟封裝在一個函數中，並使用 <code class="docutils literal notranslate"><span class="pre">&#64;tf.function</span></code> 修飾符對函數進行修飾。當需要運行此計算圖時，只需呼叫修飾後的函數即可。由此，我們可以將以上程式碼改寫如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># 以下被 @tf.function 修飾的函數定義了一個計算圖</span>
<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">graph</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">c</span>
<span class="c1"># 到此為止，計算圖定義完畢。由於 graph() 是一個函數，在其被呼叫之前，程式是不會進行任何實質計算的。</span>
<span class="c1"># 只有呼叫函數，才能通過函數取得回傳值，取得 c = 2 的結果</span>

<span class="n">c_</span> <span class="o">=</span> <span class="n">graph</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c_</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
<div class="admonition- admonition">
<p class="admonition-title">小結</p>
<ul class="simple">
<li><p>在 TensorFlow 1.X 的 API 中，我們直接在主程序中建立計算圖。而在 TensorFlow 2 中，計算圖的建立需要被封裝在一個被 <code class="docutils literal notranslate"><span class="pre">&#64;tf.function</span></code> 修飾的函數中；</p></li>
<li><p>在 TensorFlow 1.X 的 API 中，我們通過實例化一個 <code class="docutils literal notranslate"><span class="pre">tf.Session</span></code> ，並使用其 <code class="docutils literal notranslate"><span class="pre">run</span></code> 方法執行計算圖的實際運算。而在 TensorFlow 2 中，我們通過直接呼叫被 <code class="docutils literal notranslate"><span class="pre">&#64;tf.function</span></code> 修飾的函數來執行實際運算。</p></li>
</ul>
</div>
</div>
<div class="section" id="id2">
<h3>計算圖中的占位符與資料輸入<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h3>
<p>上面這個程序只能計算1+1，以下程式碼通過 TensorFlow 1.X 的圖執行模式 API 中的 <code class="docutils literal notranslate"><span class="pre">tf.placeholder()</span></code> （占位符張量）和 <code class="docutils literal notranslate"><span class="pre">sess.run()</span></code> 的 <code class="docutils literal notranslate"><span class="pre">feed_dict</span></code> 參數，展示了如何使用TensorFlow計算任意兩個數的和：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow.compat.v1</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">disable_eager_execution</span><span class="p">()</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>  <span class="c1"># 定義一個字符串Tensor</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">a_</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="p">(</span><span class="s2">&quot;a = &quot;</span><span class="p">))</span>  <span class="c1"># 從使用者讀入一個整數並放入變數a_</span>
<span class="n">b_</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="p">(</span><span class="s2">&quot;b = &quot;</span><span class="p">))</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">c_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">a</span><span class="p">:</span> <span class="n">a_</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">b_</span><span class="p">})</span>  <span class="c1"># feed_dict參數傳入為了計算c所需要的變數的值</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a + b = </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">c_</span><span class="p">)</span>
</pre></div>
</div>
<p>運行程序:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="mi">3</span>
<span class="go">a + b = 5</span>
</pre></div>
</div>
<p>而在 TensorFlow 2 中，我們可以通過為函數指定參數來實現與占位符張量相同的功能。為了在計算圖運行時送入占位符資料，只需在呼叫被修飾後的函數時，將資料作為參數傳入即可。由此，我們可以將以上程式碼改寫如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">graph</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="n">a_</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="p">(</span><span class="s2">&quot;a = &quot;</span><span class="p">))</span>
<span class="n">b_</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="p">(</span><span class="s2">&quot;b = &quot;</span><span class="p">))</span>
<span class="n">c_</span> <span class="o">=</span> <span class="n">graph</span><span class="p">(</span><span class="n">a_</span><span class="p">,</span> <span class="n">b_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a + b = </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">c_</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition- admonition">
<p class="admonition-title">小結</p>
<p>在 TensorFlow 1.X 的 API 中，我們使用 <code class="docutils literal notranslate"><span class="pre">tf.placeholder()</span></code> 在計算圖中宣告占位符張量，並通過 <code class="docutils literal notranslate"><span class="pre">sess.run()</span></code> 的 <code class="docutils literal notranslate"><span class="pre">feed_dict</span></code> 參數向計算圖中的占位符傳入實際資料。而在 TensorFlow 2 中，我們使用 <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> 的函數參數作為占位符張量，通過向被 <code class="docutils literal notranslate"><span class="pre">&#64;tf.function</span></code> 修飾的函數傳遞參數，來為計算圖中的占位符張量提供實際資料。</p>
</div>
</div>
<div class="section" id="id3">
<h3>計算圖中的變數<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h3>
<div class="section" id="id4">
<h4>變數的宣告<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h4>
<p><strong>變數</strong> （Variable）是一種特殊類型的張量，在 TensorFlow 1.X 的圖執行模式 API 中使用 <code class="docutils literal notranslate"><span class="pre">tf.get_variable()</span></code> 建立。與程式語言中的變數很相似。使用變數前需要先初始化，變數內存儲的值可以在計算圖的計算過程中被修改。以下示例程式碼展示了如何建立一個變數，將其值初始化為0，並逐次累加1。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow.compat.v1</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">disable_eager_execution</span><span class="p">()</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[])</span>
<span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>   <span class="c1"># tf.assign(x, y)返回一個“將變數y的值指定給變數x”的操作</span>
<span class="n">plus_one_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">plus_one_op</span><span class="p">)</span>       <span class="c1"># 對變數a執行此一操作</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>          <span class="c1"># 輸出此時變數a在當前會話的計算圖的值</span>
</pre></div>
</div>
<p>輸出:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.0</span>
<span class="mf">2.0</span>
<span class="mf">3.0</span>
<span class="mf">4.0</span>
<span class="mf">5.0</span>
</pre></div>
</div>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p>為了初始化變數，也可以在宣告變數時指定初始化器（initializer），並通過 <code class="docutils literal notranslate"><span class="pre">tf.global_variables_initializer()</span></code> 一次性初始化所有變數，在實際工程中更常用：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow.compat.v1</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">disable_eager_execution</span><span class="p">()</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> 
    <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>   <span class="c1"># 指定初始化器為全0初始化</span>
<span class="n">plus_one_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span> <span class="c1"># 初始化所有變數</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">plus_one_op</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>在 TensorFlow 2 中，我們通過實例化 <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> 類來宣告變數。由此，我們可以將以上程式碼改寫如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">plus_one_op</span><span class="p">():</span>
    <span class="n">a</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">plus_one_op</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
<div class="admonition- admonition">
<p class="admonition-title">小結</p>
<p>在 TensorFlow 1.X 的 API 中，我們使用 <code class="docutils literal notranslate"><span class="pre">tf.get_variable()</span></code> 在計算圖中宣告變數節點。而在 TensorFlow 2 中，我們直接通過 <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> 實例化變數對象，並在計算圖中使用這一變數對象。</p>
</div>
</div>
<div class="section" id="id5">
<h4>變數的作用域與重用<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h4>
<p>在 TensorFlow 1.X 中，我們建立模型時經常需要指定變數的作用域，以及複用變數。此時，TensorFlow 1.X 的圖執行模式 API 為我們提供了 <code class="docutils literal notranslate"><span class="pre">tf.variable_scope()</span></code> 及 <code class="docutils literal notranslate"><span class="pre">reuse</span></code> 參數來實現變數作用域和複用變數的功能。以下的例子使用了 TensorFlow 1.X 的圖執行模式 API 建立了一個三層的全連接神經網絡，其中第三層複用了第二層的變數。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow.compat.v1</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">tf</span><span class="o">.</span><span class="n">disable_eager_execution</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">dense</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">num_units</span><span class="p">):</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_units</span><span class="p">])</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">num_units</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;dense1&#39;</span><span class="p">):</span>   <span class="c1"># 限定變數的作用為 dense1</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>           <span class="c1"># 宣告 dense1/weight 和 dense1/bias 兩個變量</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;dense2&#39;</span><span class="p">):</span>   <span class="c1"># 限定變數的作用為 dense2</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>                <span class="c1"># 宣告 dense2/weight 和 dense2/bias 兩個變數</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;dense2&#39;</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>   <span class="c1"># 第三層重複使用第二層的變數</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables</span><span class="p">())</span>    <span class="c1"># 輸出當前計算圖中的所有變數節點</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
<span class="n">outputs_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">inputs</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">)})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">outputs_</span><span class="p">)</span>
</pre></div>
</div>
<p>在上例中，計算圖的所有變數節點為：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="o">&lt;</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span> <span class="s1">&#39;dense1/weight:0&#39;</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="o">&gt;</span><span class="p">,</span>
 <span class="o">&lt;</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span> <span class="s1">&#39;dense1/bias:0&#39;</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,)</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="o">&gt;</span><span class="p">,</span>
 <span class="o">&lt;</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span> <span class="s1">&#39;dense2/weight:0&#39;</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="o">&gt;</span><span class="p">,</span>
 <span class="o">&lt;</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span> <span class="s1">&#39;dense2/bias:0&#39;</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,)</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="o">&gt;</span><span class="p">]</span>
</pre></div>
</div>
<p>可見， <code class="docutils literal notranslate"><span class="pre">tf.variable_scope()</span></code> 為在其上下文中的，以 <code class="docutils literal notranslate"><span class="pre">tf.get_variable</span></code> 建立的變數的名稱添加了「前綴」或「作用域」，使得變數在計算圖中的層次結構更為清晰，不同「作用域」下的同名變數各司其職，不會衝突。同時，雖然我們在上例中呼叫了3次 <code class="docutils literal notranslate"><span class="pre">dense</span></code> 函數，即呼叫了6次 <code class="docutils literal notranslate"><span class="pre">tf.get_variable</span></code> 函數，但實際建立的變數節點只有4個。這即是 <code class="docutils literal notranslate"><span class="pre">tf.variable_scope()</span></code> 的 <code class="docutils literal notranslate"><span class="pre">reuse</span></code> 參數所起到的作用。當 <code class="docutils literal notranslate"><span class="pre">reuse=True</span></code> 時， <code class="docutils literal notranslate"><span class="pre">tf.get_variable</span></code> 遇到重名變數時將會自動獲取先前建立的同名變數，而不會新建變數，從而達到了變數重用的目的。</p>
<p>而在 TensorFlow 2 的圖執行模式 API 中，不再鼓勵使用 <code class="docutils literal notranslate"><span class="pre">tf.variable_scope()</span></code> ，而應當使用 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code> 和  <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 來封裝程式碼和指定作用域，具體可參考 <a class="reference internal" href="../basic/models.html"><span class="doc">本手冊第三章</span></a>。上面的例子與下面基於 <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> 和 <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> 的程式碼等價。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_units</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_units</span> <span class="o">=</span> <span class="n">num_units</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">y_pred</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">num_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dense1&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">num_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dense2&#39;</span><span class="p">)</span>

    <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">)))</span>
</pre></div>
</div>
<p>我們可以注意到，在 TensorFlow 2 中，變數的作用域以及複用變數的問題自然地淡化了。基於Python類的模型建立方式自然地為變數指定了作用域，而變數的重用也可以通過簡單地多次呼叫同一個層來實現。</p>
<p>為了詳細了解上面的程式碼對變數作用域的處理方式，我們使用 <code class="docutils literal notranslate"><span class="pre">get_concrete_function</span></code> 導出計算圖，並輸出計算圖中的所有變數節點：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">call</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
</pre></div>
</div>
<p>輸出如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="o">&lt;</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span> <span class="s1">&#39;dense1/weight:0&#39;</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">numpy</span><span class="o">=...&gt;</span><span class="p">,</span>
 <span class="o">&lt;</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span> <span class="s1">&#39;dense1/bias:0&#39;</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,)</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">numpy</span><span class="o">=...&gt;</span><span class="p">,</span>
 <span class="o">&lt;</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span> <span class="s1">&#39;dense2/weight:0&#39;</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">numpy</span><span class="o">=...&gt;</span><span class="p">,</span>
 <span class="o">&lt;</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span> <span class="s1">&#39;dense2/bias:0&#39;</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,)</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">numpy</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<p>可見，TensorFlow 2 的圖執行模式在變數的作用域上與 TensorFlow 1.X 實際保持了一致。我們通過 <code class="docutils literal notranslate"><span class="pre">name</span></code> 參數為每個層指定的名稱將成為層內變數的作用域。</p>
<div class="admonition- admonition">
<p class="admonition-title">小結</p>
<p>在 TensorFlow 1.X 的 API 中，使用 <code class="docutils literal notranslate"><span class="pre">tf.variable_scope()</span></code> 及 <code class="docutils literal notranslate"><span class="pre">reuse</span></code> 參數來實現變數作用域和複用變數的功能。在 TensorFlow 2 中，使用 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code> 和  <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 來封裝程式碼和指定作用域，從而使變數的作用域以及複用變數的問題自然淡化。兩者的實質是一樣的。</p>
</div>
</div>
</div>
</div>
<div class="section" id="id6">
<h2>自動求導機制與優化器<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h2>
<p>在本節中，我們對 TensorFlow 1.X 和 TensorFlow 2 在圖執行模式下的自動求導機制進行較深入的比較說明。</p>
<div class="section" id="id7">
<h3>自動求導機制<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h3>
<p>我們首先回顧 TensorFlow 1.X 中的自動求導機制。在 TensorFlow 1.X 的圖執行模式 API 中，可以使用 <code class="docutils literal notranslate"><span class="pre">tf.gradients(y,</span> <span class="pre">x)</span></code> 計算計算圖中的張量節點 <code class="docutils literal notranslate"><span class="pre">y</span></code> 相對於變數 <code class="docutils literal notranslate"><span class="pre">x</span></code> 的導數。以下示例展示了在 TensorFlow 1.X 的圖執行模式 API 中計算 <img class="math" src="../../_images/math/310ab0ef1ada6edb2b67c308ecc0af9d5a3b36f1.png" alt="y = x^2"/> 在 <img class="math" src="../../_images/math/6242bf7ca264a34d82d13f3307438c9bd77b8e15.png" alt="x = 3"/> 時的導數。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">3.</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># y = x ^ 2</span>
<span class="n">y_grad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>以上程式碼中，計算圖中的節點 <code class="docutils literal notranslate"><span class="pre">y_grad</span></code> 即為 <code class="docutils literal notranslate"><span class="pre">y</span></code> 相對於 <code class="docutils literal notranslate"><span class="pre">x</span></code> 的導數。</p>
<p>而在 TensorFlow 2 的圖執行模式 API 中，我們使用 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 這一上下文管理器封裝需要求導的計算步驟，並使用其 <code class="docutils literal notranslate"><span class="pre">gradient</span></code> 方法求導，程式碼示例如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">3.</span><span class="p">)</span>
<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">grad</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y_grad</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y_grad</span>
</pre></div>
</div>
<div class="admonition- admonition">
<p class="admonition-title">小結</p>
<p>在 TensorFlow 1.X 中，我們使用 <code class="docutils literal notranslate"><span class="pre">tf.gradients()</span></code> 求導。而在 TensorFlow 2 中，我們使用使用 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 這一上下文管理器封裝需要求導的計算步驟，並使用其 <code class="docutils literal notranslate"><span class="pre">gradient</span></code> 方法求導。</p>
</div>
</div>
<div class="section" id="id8">
<h3>優化器<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h3>
<p>由於機器學習中的求導往往伴隨著優化，所以 TensorFlow 中更常用的是優化器（Optimizer）。在 TensorFlow 1.X 的圖執行模式 API 中，我們往往使用 <code class="docutils literal notranslate"><span class="pre">tf.train</span></code> 中的各種優化器，將求導和調整變數值的步驟合二為一。例如，以下程式碼片段在計算圖建構過程中，使用 <code class="docutils literal notranslate"><span class="pre">tf.train.GradientDescentOptimizer</span></code> 這一梯度下降優化器優化損失函數 <code class="docutils literal notranslate"><span class="pre">loss</span></code> ：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data_placeholder</span><span class="p">)</span>    <span class="c1"># 模型建構</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>                          <span class="c1"># 計算模型的損失函數 loss</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">train_one_step</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="c1"># 上面一步也可拆分為</span>
<span class="c1"># grad = optimizer.compute_gradients(loss)</span>
<span class="c1"># train_one_step = optimizer.apply_gradients(grad)</span>
</pre></div>
</div>
<p>以上程式碼中， <code class="docutils literal notranslate"><span class="pre">train_one_step</span></code> 即為一個將求導和變數值更新合二為一的計算圖節點（操作），也就是訓練過程中的「一步」。特別需要注意的是，對於優化器的 <code class="docutils literal notranslate"><span class="pre">minimize</span></code> 方法而言，只需要指定待優化的損失函數張量節點 <code class="docutils literal notranslate"><span class="pre">loss</span></code> 即可，求導的變數可以自動從計算圖中獲得（即 <code class="docutils literal notranslate"><span class="pre">tf.trainable_variables</span></code> ）。在計算圖建構完成後，只需啓動對話，使用 <code class="docutils literal notranslate"><span class="pre">sess.run</span></code> 方法運行 <code class="docutils literal notranslate"><span class="pre">train_one_step</span></code> 這一計算圖節點，並通過 <code class="docutils literal notranslate"><span class="pre">feed_dict</span></code> 參數送入訓練資料，即可完成一步訓練。程式碼片段如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="n">data_dict</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># 將訓練所需資料放入字典 data 內</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_one_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">data_dict</span><span class="p">)</span>
</pre></div>
</div>
<p>而在 TensorFlow 2 的 API 中，無論是圖執行模式還是即時執行模式，均先使用 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 進行求導操作，然後再使用優化器的 <code class="docutils literal notranslate"><span class="pre">apply_gradients</span></code> 方法應用已求得的導數，進行變數值的更新。也就是說，和 TensorFlow 1.X 中優化器的 <code class="docutils literal notranslate"><span class="pre">compute_gradients</span></code> + <code class="docutils literal notranslate"><span class="pre">apply_gradients</span></code> 十分類似。同時，在 TensorFlow 2 中，無論是求導還是使用導數更新變數值，都需要顯式地指定變數。計算圖的建構程式碼結構如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=...</span><span class="p">)</span>

<span class="nd">@tf.function</span>
<span class="k">def</span> <span class="nf">train_one_step</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>    <span class="c1"># 模型建構</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>              <span class="c1"># 計算模型的損失函數 loss</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads_and_vars</span><span class="o">=</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">))</span>
</pre></div>
</div>
<p>在計算圖建構完成後，我們直接呼叫 <code class="docutils literal notranslate"><span class="pre">train_one_step</span></code> 函數並送入訓練資料即可：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="n">train_one_step</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition- admonition">
<p class="admonition-title">小結</p>
<p>在 TensorFlow 1.X 中，我們多使用優化器的 <code class="docutils literal notranslate"><span class="pre">minimize</span></code> 方法，將求導和變數值更新合二為一。而在 TensorFlow 2 中，我們需要先使用 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 進行求導操作，然後再使用優化器的 <code class="docutils literal notranslate"><span class="pre">apply_gradients</span></code> 方法應用已求得的導數，進行變數值的更新。而且在這兩步中，都需要顯式指定待求導和待更新的變數。</p>
</div>
</div>
<div class="section" id="zh-hant-graph-compare">
<span id="id9"></span><h3>自動求導機制的計算圖對比 *<a class="headerlink" href="#zh-hant-graph-compare" title="永久链接至标题">¶</a></h3>
<p>在本節，為了幫助讀者更深刻地理解 TensorFlow 的自動求導機制，我們以前節的「計算 <img class="math" src="../../_images/math/310ab0ef1ada6edb2b67c308ecc0af9d5a3b36f1.png" alt="y = x^2"/> 在 <img class="math" src="../../_images/math/6242bf7ca264a34d82d13f3307438c9bd77b8e15.png" alt="x = 3"/> 時的導數」為例，展示 TensorFlow 1.X 和 TensorFlow 2 在圖執行模式下，為這一求導過程所建立的計算圖，並進行詳細講解。</p>
<p>在 TensorFlow 1.X 的圖執行模式 API 中，將生成的計算圖使用 TensorBoard 進行展示：</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/grad_v1.png"><img alt="../../_images/grad_v1.png" src="../../_images/grad_v1.png" style="width: 60%;" /></a>
</div>
<p>在計算圖中，灰色的塊為節點的命名空間（Namespace，後文簡稱「塊」），橢圓形代表操作節點（OpNode），圓形代表常量，灰色的箭頭代表資料流。為了弄清計算圖節點 <code class="docutils literal notranslate"><span class="pre">x</span></code> 、 <code class="docutils literal notranslate"><span class="pre">y</span></code> 和 <code class="docutils literal notranslate"><span class="pre">y_grad</span></code> 與計算圖中節點的對應關係，我們將這些變數節點輸出，可見：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x</span></code> : <code class="docutils literal notranslate"><span class="pre">&lt;tf.Variable</span> <span class="pre">'x:0'</span> <span class="pre">shape=()</span> <span class="pre">dtype=float32&gt;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y</span></code> : <code class="docutils literal notranslate"><span class="pre">Tensor(&quot;Square:0&quot;,</span> <span class="pre">shape=(),</span> <span class="pre">dtype=float32)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_grad</span></code> : <code class="docutils literal notranslate"><span class="pre">[&lt;tf.Tensor</span> <span class="pre">'gradients/Square_grad/Mul_1:0'</span> <span class="pre">shape=()</span> <span class="pre">dtype=float32&gt;]</span></code></p></li>
</ul>
<p>在 TensorBoard 中，我們也可以通過點擊節點獲得節點名稱。通過比較我們可以得知，變數 <code class="docutils literal notranslate"><span class="pre">x</span></code> 對應計算圖最下方的x，節點 <code class="docutils literal notranslate"><span class="pre">y</span></code> 對應計算圖「Square」塊的「 <code class="docutils literal notranslate"><span class="pre">(Square)</span></code> 」，節點 <code class="docutils literal notranslate"><span class="pre">y_grad</span></code> 對應計算圖上方「Square_grad」的 <code class="docutils literal notranslate"><span class="pre">Mul_1</span></code> 節點。同時我們還可以通過點擊節點發現，「Square_grad」塊里的const節點值為2，「gradients」塊里的 <code class="docutils literal notranslate"><span class="pre">grad_ys_0</span></code> 值為1， <code class="docutils literal notranslate"><span class="pre">Shape</span></code> 值為空，以及「x」塊的const節點值為3。</p>
<p>接下來，我們開始具體分析這個計算圖的結構。我們可以注意到，這個計算圖的結構是比較清晰的，「x」塊負責變數的讀取和初始化，「Square」塊負責求平方 <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">^</span> <span class="pre">2</span></code> ，而「gradients」塊則負責對「Square」塊的操作求導，即計算 <code class="docutils literal notranslate"><span class="pre">y_grad</span> <span class="pre">=</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">x</span></code>。由此我們可以看出， <code class="docutils literal notranslate"><span class="pre">tf.gradients</span></code> 是一個相對比較「龐大」的操作，並非如一般的操作一樣往計算圖中添加了一個或幾個節點，而是建立了一個龐大的子圖，以應用鏈式法則求計算圖中特定節點的導數。</p>
<p>在 TensorFlow 2 的圖執行模式 API 中，將生成的計算圖使用 TensorBoard 進行展示：</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/grad_v2.png"><img alt="../../_images/grad_v2.png" src="../../_images/grad_v2.png" style="width: 60%;" /></a>
</div>
<p>我們可以注意到，除了求導過程沒有封裝在「gradients」塊內，以及變數的處理簡化以外，其他的區別並不大。由此，我們可以看出，在圖執行模式下， <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 這一上下文管理器的 <code class="docutils literal notranslate"><span class="pre">gradient</span></code> 方法和 TensorFlow 1.X 的 <code class="docutils literal notranslate"><span class="pre">tf.gradients</span></code> 是基本等價的。</p>
<div class="admonition- admonition">
<p class="admonition-title">小結</p>
<p>TensorFlow 1.X 中的 <code class="docutils literal notranslate"><span class="pre">tf.gradients</span></code> 和 TensorFlow 2 圖執行模式下的  <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 上下文管理器儘管在 API 層面的呼叫方法略有不同，但最終生成的計算圖是基本一致的。</p>
</div>
</div>
</div>
<div class="section" id="id10">
<h2>基礎示例：線性回歸<a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h2>
<p>在本節，我們為 <span class="xref std std-ref">第一章的線性回歸示例</span> 提供一個基於 TensorFlow 1.X 的圖執行模式 API 的版本，供有需要的讀者對比參考。</p>
<p>與第一章的NumPy和即時執行模式不同，TensorFlow的圖執行模式使用 <strong>符號式編程</strong> 來進行數值運算。首先，我們需要將待計算的過程抽象為計算圖，將輸入、運算和輸出都用符號化的節點來表達。然後，我們將資料不斷地送入輸入節點，讓資料沿著計算圖進行計算和流動，最終到達我們需要的特定輸出節點。</p>
<p>以下程式碼展示了如何基於 TensorFlow 的符號編譯方法完成與前節相同的任務。其中， <code class="docutils literal notranslate"><span class="pre">tf.placeholder()</span></code> 即可以視為一種「符號化的輸入節點」，使用 <code class="docutils literal notranslate"><span class="pre">tf.get_variable()</span></code> 定義模型的參數（Variable類型的張量可以使用 <code class="docutils literal notranslate"><span class="pre">tf.assign()</span></code> 操作進行賦值），而 <code class="docutils literal notranslate"><span class="pre">sess.run(output_node,</span> <span class="pre">feed_dict={input_node:</span> <span class="pre">data})</span></code> 可以視作將資料送入輸入節點，沿著計算圖計算併到達輸出節點並返回值的過程。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># 定義資料流圖</span>
<span class="n">learning_rate_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">X_</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_</span><span class="p">))</span>

<span class="c1"># 反向傳播，手動計算變數（模型參數）的梯度</span>
<span class="n">grad_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_</span><span class="p">)</span>
<span class="n">grad_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_</span><span class="p">)</span>

<span class="c1"># 梯度下降法，手動更新參數</span>
<span class="n">new_a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">-</span> <span class="n">learning_rate_</span> <span class="o">*</span> <span class="n">grad_a</span>
<span class="n">new_b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">learning_rate_</span> <span class="o">*</span> <span class="n">grad_b</span>
<span class="n">update_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">new_a</span><span class="p">)</span>
<span class="n">update_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">new_b</span><span class="p">)</span>

<span class="n">train_op</span> <span class="o">=</span> <span class="p">[</span><span class="n">update_a</span><span class="p">,</span> <span class="n">update_b</span><span class="p">]</span> 
<span class="c1"># 資料流圖定義到此結束</span>
<span class="c1"># 注意，直到目前，我們都沒有進行任何實質的資料計算，僅僅是定義了一個資料流圖</span>

<span class="n">num_epoch</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c1"># 初始化變數a和b</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="c1"># 迴圈將資料送入上面建立的資料流圖中進行計算和更新變數</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epoch</span><span class="p">):</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X_</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="n">y_</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate_</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">]))</span>
</pre></div>
</div>
<div class="section" id="id11">
<h3>自動求導機制<a class="headerlink" href="#id11" title="永久链接至标题">¶</a></h3>
<p>在上面的兩個示例中，我們都是人工計算獲得損失函數關於各參數的偏導數。但當模型和損失函數都變得十分複雜時（尤其是深度學習模型），這種人工求導的工作量就難以接受了。因此，在圖執行模式中，TensorFlow同樣提供了 <strong>自動求導機制</strong> 。類似於即時執行模式下的 <code class="docutils literal notranslate"><span class="pre">tape.grad(ys,</span> <span class="pre">xs)</span></code> ，可以利用TensorFlow的求導操作 <code class="docutils literal notranslate"><span class="pre">tf.gradients(ys,</span> <span class="pre">xs)</span></code> 求出損失函數 <code class="docutils literal notranslate"><span class="pre">loss</span></code> 關於 <code class="docutils literal notranslate"><span class="pre">a</span></code> ， <code class="docutils literal notranslate"><span class="pre">b</span></code> 的偏導數。由此，我們可以將上節中的兩行手工計算導數的程式碼</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 反向傳播，手動計算變數（模型參數）的梯度</span>
<span class="n">grad_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_</span><span class="p">)</span>
<span class="n">grad_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_</span><span class="p">)</span>
</pre></div>
</div>
<p>替換為</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
</pre></div>
</div>
<p>計算結果將不會改變。</p>
</div>
<div class="section" id="id12">
<h3>優化器<a class="headerlink" href="#id12" title="永久链接至标题">¶</a></h3>
<p>TensorFlow在圖執行模式下也附帶有多種 <strong>優化器</strong> （optimizer），可以將求導和梯度更新一併完成。我們可以將上節的程式碼</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 反向傳播，手動計算變數（模型參數）的梯度</span>
<span class="n">grad_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_</span><span class="p">)</span>
<span class="n">grad_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_</span><span class="p">)</span>

<span class="c1"># 梯度下降法，手動更新參數</span>
<span class="n">new_a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">-</span> <span class="n">learning_rate_</span> <span class="o">*</span> <span class="n">grad_a</span>
<span class="n">new_b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">learning_rate_</span> <span class="o">*</span> <span class="n">grad_b</span>
<span class="n">update_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">new_a</span><span class="p">)</span>
<span class="n">update_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">new_b</span><span class="p">)</span>

<span class="n">train_op</span> <span class="o">=</span> <span class="p">[</span><span class="n">update_a</span><span class="p">,</span> <span class="n">update_b</span><span class="p">]</span> 
</pre></div>
</div>
<p>整體替換為</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate_</span><span class="p">)</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
<p>這裡，我們先實例化了一個TensorFlow中的梯度下降優化器 <code class="docutils literal notranslate"><span class="pre">tf.train.GradientDescentOptimizer()</span></code> 並設置學習率。然後利用其 <code class="docutils literal notranslate"><span class="pre">compute_gradients(loss)</span></code> 方法求出 <code class="docutils literal notranslate"><span class="pre">loss</span></code> 對所有變數（參數）的梯度。最後通過 <code class="docutils literal notranslate"><span class="pre">apply_gradients(grad)</span></code> 方法，根據前面算出的梯度來梯度下降更新變數（參數）。</p>
<p>以上三行程式碼等價於下面一行程式碼：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate_</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>使用自動求導機制和優化器簡化後的程式碼如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">learning_rate_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">X_</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_</span><span class="p">))</span>

<span class="c1"># 反向传播，利用TensorFlow的梯度下降优化器自动计算并更新变量（模型参数）的梯度</span>
<span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate_</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">num_epoch</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epoch</span><span class="p">):</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X_</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="n">y_</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate_</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2021, Xihan Li (snowkylin)

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.

  <p><a href="https://beian.miit.gov.cn/" target="_blank">沪ICP备13038357号-18</a ></p> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-40509304-12', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>