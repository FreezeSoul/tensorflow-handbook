

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tf.GradientTape 詳解 &mdash; 简单粗暴 TensorFlow 2 0.4 beta 文档</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../_static/js/tw_cn.js"></script>
        <script type="text/javascript" src="../../_static/js/pangu.min.js"></script>
        <script type="text/javascript" src="../../_static/js/custom_20200921.js"></script>
        <script type="text/javascript" src="../../_static/translations.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> 简单粗暴 TensorFlow 2
          

          
          </a>

          
            
            
              <div class="version">
                0.4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">目录</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/foreword.html">推荐序</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/preface.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/introduction.html">TensorFlow概述</a></li>
</ul>
<p class="caption"><span class="caption-text">基础</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/installation.html">TensorFlow安装与环境配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/basic.html">TensorFlow基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/models.html">TensorFlow 模型建立与训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/basic/tools.html">TensorFlow常用模块</a></li>
</ul>
<p class="caption"><span class="caption-text">部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/export.html">TensorFlow模型导出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/serving.html">TensorFlow Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/lite.html">TensorFlow Lite（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/deployment/javascript.html">TensorFlow in JavaScript（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">大规模训练与加速</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/distributed.html">TensorFlow分布式训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/tpu.html">使用TPU训练TensorFlow模型（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">扩展</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/tfhub.html">TensorFlow Hub 模型复用（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/tfds.html">TensorFlow Datasets 数据集载入</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/swift.html">Swift for TensorFlow (S4TF) (Huan）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/quantum.html">TensorFlow Quantum: 混合量子-经典机器学习 *</a></li>
</ul>
<p class="caption"><span class="caption-text">附录</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/rl.html">强化学习简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/docker.html">使用Docker部署TensorFlow环境</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/cloud.html">在云端使用TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/jupyterlab.html">部署自己的交互式Python开发环境JupyterLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/recommended_books.html">参考资料与推荐阅读</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh_hans/appendix/terms.html">术语中英对照表</a></li>
</ul>
<p class="caption"><span class="caption-text">目錄</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../preface.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">TensorFlow概述</a></li>
</ul>
<p class="caption"><span class="caption-text">基礎</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../basic/installation.html">TensorFlow 安裝與環境配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/basic.html">TensorFlow 基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/models.html">TensorFlow 模型建立與訓練</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/tools.html">TensorFlow常用模組</a></li>
</ul>
<p class="caption"><span class="caption-text">部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deployment/export.html">TensorFlow模型匯出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/serving.html">TensorFlow Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/lite.html">TensorFlow Lite（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/javascript.html">TensorFlow in JavaScript（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">大規模訓練與加速</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix/distributed.html">TensorFlow分布式訓練</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/tpu.html">使用TPU訓練TensorFlow模型（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">擴展</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix/tfhub.html">TensorFlow Hub 模型複用（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/tfds.html">TensorFlow Datasets 資料集載入</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/swift.html">Swift for TensorFlow (S4TF) (Huan）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/quantum.html">TensorFlow Quantum: 混合量子-經典機器學習 *</a></li>
</ul>
<p class="caption"><span class="caption-text">附錄</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix/rl.html">強化學習簡介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/docker.html">使用Docker部署TensorFlow環境</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/cloud.html">在雲端使用TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/jupyterlab.html">部署自己的互動式 Python 開發環境 JupyterLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/recommended_books.html">參考資料與推薦閱讀</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/terms.html">專有名詞中英對照表</a></li>
</ul>
<p class="caption"><span class="caption-text">Preface</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/introduction.html">TensorFlow Overview</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/installation.html">Installation and Environment Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/basic.html">TensorFlow Basic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/models.html">Model Construction and Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/basic/tools.html">Common Modules in TensorFlow</a></li>
</ul>
<p class="caption"><span class="caption-text">Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/deployment/export.html">TensorFlow Model Export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/deployment/serving.html">TensorFlow Serving</a></li>
</ul>
<p class="caption"><span class="caption-text">Large-scale Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/appendix/distributed.html">Distributed training with TensorFlow</a></li>
</ul>
<p class="caption"><span class="caption-text">Extensions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../en/appendix/tfds.html">TensorFlow Datasets: Ready-to-use Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../en/appendix/quantum.html">TensorFlow Quantum: Hybrid Quantum-classical Machine Learning *</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">简单粗暴 TensorFlow 2</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li><code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 詳解</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/zh_hant/advanced/tape.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tf-gradienttape">
<h1><code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 詳解<a class="headerlink" href="#tf-gradienttape" title="永久链接至标题">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 的出現是 TensorFlow 2 最大的變化之一。其以一種簡潔優雅的方式，爲 TensorFlow 的即時執行模式和圖執行模式提供了統一的自動求導 API。不過對於從 TensorFlow 1.X 過渡到 TensorFlow 2 的開發人員而言，也增加了一定的學習門檻。本章即在 <span class="xref std std-ref">第一章「自動求導機制」一節</span> 的基礎上，詳細介紹 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 的使用方法及機制。</p>
<div class="section" id="id1">
<h2>基本使用<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 是一個記錄器，能夠記錄在其上下文環境中的計算步驟和操作，並用於自動求導。其使用方法分爲兩步：</p>
<ol class="arabic simple">
<li><p>使用 with 語句，將需要求導的計算步驟封裝在 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 的上下文中；</p></li>
<li><p>使用 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 的 <code class="docutils literal notranslate"><span class="pre">gradient</span></code> 方法計算導數。</p></li>
</ol>
<p>回顧 <span class="xref std std-ref">第一章「自動求導機制」一節</span> 所舉的例子，使用 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape()</span></code> 計算函數 <img class="math" src="../../_images/math/2261dfa63cebc11353c25f3b42ab8812925703e7.png" alt="y(x) = x^2"/> 在 <img class="math" src="../../_images/math/6242bf7ca264a34d82d13f3307438c9bd77b8e15.png" alt="x = 3"/> 時的導數：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mf">3.</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>     <span class="c1"># 在 tf.GradientTape() 的上下文内，所有计算步骤都会被记录以用于求导</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y_grad</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>        <span class="c1"># 计算y关于x的导数</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_grad</span><span class="p">)</span>
</pre></div>
</div>
<p>在這裡，初學者往往迷惑於此處 with 語句的用法，即「爲什麼離開了上下文環境， <code class="docutils literal notranslate"><span class="pre">tape</span></code> 還可以被使用？」。這樣的疑惑是有一定道理的，因爲在實際應用中，with 語句大多用於對資源進行訪問的場合，保證資源在使用後得到恰當的清理或釋放，例如我們熟悉的文件寫入：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;test.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>    <span class="c1"># open() 是文件资源的上下文管理器，f 是文件资源对象</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;hello world&#39;</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;another string&#39;</span><span class="p">)</span>   <span class="c1"># 报错，因为离开上下文环境时，资源对象 f 被其上下文管理器所释放</span>
</pre></div>
</div>
<p>在 TensorFlow 2 中，<code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 儘管也可以被視爲一種「資源」的上下文管理器，但和傳統的資源有所區別。傳統的資源在進入上下文管理器時獲取資源對象，離開時釋放資源對象，因此在離開上下文環境後再訪問資源對象往往無效。而 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 則是在進入上下文管理器時新建記錄器並開啓記錄，離開上下文管理器時讓記錄器停止記錄。停止記錄不代表記錄器被釋放，事實上，記錄器所記錄的信息仍然保留，只是不再記錄新的信息。因此 <code class="docutils literal notranslate"><span class="pre">tape</span></code> 的 <code class="docutils literal notranslate"><span class="pre">gradient</span></code> 方法依然可以使用，以利用已記錄的信息計算導數。我們使用以下示例代碼來說明這一點：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mf">3.</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>     <span class="c1"># tf.GradientTape() 是上下文管理器，tape 是记录器</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tape</span><span class="o">.</span><span class="n">stop_recording</span><span class="p">():</span>     <span class="c1"># 在上下文管理器内，记录进行中，暂时停止记录成功</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;temporarily stop recording&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tape</span><span class="o">.</span><span class="n">stop_recording</span><span class="p">():</span>         <span class="c1"># 在上下文管理器外，记录已停止，尝试暂时停止记录报错</span>
    <span class="k">pass</span>
<span class="n">y_grad</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>        <span class="c1"># 在上下文管理器外，tape 的记录信息仍然保留，导数计算成功</span>
</pre></div>
</div>
<p>在以上代碼中， <code class="docutils literal notranslate"><span class="pre">tape.stop_recording()</span></code> 上下文管理器可以暫停計算步驟的記錄。也就是說，在該上下文內的計算步驟都無法使用  <code class="docutils literal notranslate"><span class="pre">tape</span></code> 的 <code class="docutils literal notranslate"><span class="pre">gradient</span></code> 方法求導。在第一次調用 <code class="docutils literal notranslate"><span class="pre">tape.stop_recording()</span></code> 時， <code class="docutils literal notranslate"><span class="pre">tape</span></code> 是處於記錄狀態的，因此調用成功。而第二次調用 <code class="docutils literal notranslate"><span class="pre">tape.stop_recording()</span></code> 時，由於 <code class="docutils literal notranslate"><span class="pre">tape</span></code> 已經離開了 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 上下文，在離開時 <code class="docutils literal notranslate"><span class="pre">tape</span></code> 的記錄狀態被停止，所以調用失敗，報錯： <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">Tape</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">recording.</span></code> （記錄器已經停止記錄）。</p>
</div>
<div class="section" id="id2">
<h2>監視機制<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<p>在 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 中，通過監視（Watch）機制來決定 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 可以對哪些變量求導。默認情況下，可訓練（Trainable）的變量，如 <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> 會被自動加入 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 的監視列表，從而 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 可以直接對這些變量求導。而另一些類型的張量（例如 <code class="docutils literal notranslate"><span class="pre">tf.Constant</span></code> ）則不在默認列表中，若需要對這些張量求導，需要使用 <code class="docutils literal notranslate"><span class="pre">watch</span></code> 方法手工將張量加入監視列表中。以下示例代碼說明了這一點：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">3.</span><span class="p">)</span>                 <span class="c1"># x 为常量类型张量，默认无法对其求导</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y_grad_1</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>      <span class="c1"># 求导结果为 None</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                   <span class="c1"># 使用 tape.watch 手动将 x 加入监视列表</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y_grad_2</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>      <span class="c1"># 求导结果为 tf.Tensor(6.0, shape=(), dtype=float32)</span>
</pre></div>
</div>
<p>當然，如果你希望自己掌控需要監視的變量，可以將 <code class="docutils literal notranslate"><span class="pre">watch_accessed_variables=False</span></code> 選項傳入 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> ，並使用 <code class="docutils literal notranslate"><span class="pre">watch</span></code> 方法手動逐個加入需要監視的變量。</p>
</div>
<div class="section" id="id3">
<h2>高階求導<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 支持嵌套使用。通過嵌套 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 上下文管理器，可以輕鬆地實現二階、三階甚至更多階的求導。以下示例代碼計算了 <img class="math" src="../../_images/math/2261dfa63cebc11353c25f3b42ab8812925703e7.png" alt="y(x) = x^2"/> 在 <img class="math" src="../../_images/math/6242bf7ca264a34d82d13f3307438c9bd77b8e15.png" alt="x = 3"/> 時的一階導數 <code class="docutils literal notranslate"><span class="pre">dy_dx</span></code> 和二階導數 <code class="docutils literal notranslate"><span class="pre">d2y_dx2</span></code> ：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">3.</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape_1</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape_2</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">dy_dx</span> <span class="o">=</span> <span class="n">tape_2</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>   <span class="c1"># 值为 6.0</span>
<span class="n">d2y_dx2</span> <span class="o">=</span> <span class="n">tape_1</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">dy_dx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># 值为 2.0</span>
</pre></div>
</div>
<p>由於 <img class="math" src="../../_images/math/76e17810ac75959a92d4bb2465a71e4472696774.png" alt="\frac{dy}{dx} = 2x"/> ， <img class="math" src="../../_images/math/bd7d4b9c7e5643075cbeb96dc2f3ccdb97fac53c.png" alt="\frac{d^2y}{dx^2} = \frac{d}{dx}\frac{dy}{dx} = 2"/> ，故期望值爲 <code class="docutils literal notranslate"><span class="pre">dy_dx</span> <span class="pre">=</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">3</span> <span class="pre">=</span> <span class="pre">6</span></code> ， <code class="docutils literal notranslate"><span class="pre">d2y_dx2</span> <span class="pre">=</span> <span class="pre">2</span></code> ，可見實際計算值與預期相符。</p>
<p>我們可以從上面的代碼看出，高階求導實際上是通過對使用 <code class="docutils literal notranslate"><span class="pre">tape</span></code> 的 <code class="docutils literal notranslate"><span class="pre">gradient</span></code> 方法求得的導數繼續求導來實現的。也就是說，求導操作（即 <code class="docutils literal notranslate"><span class="pre">tape</span></code> 的 <code class="docutils literal notranslate"><span class="pre">gradient</span></code> 方法）和其他計算步驟（如 <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">tf.square(x)</span></code> ）沒有什麼本質的不同，同樣是可以被 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 記錄的計算步驟。</p>
</div>
<div class="section" id="id4">
<h2>持久保持記錄與多次求導<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h2>
<p>默認情況下，每個 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 的記錄器在調用一次 <code class="docutils literal notranslate"><span class="pre">gradient</span></code> 方法後，其記錄的信息就會被釋放，也就是說這個記錄器就無法再使用了。但如果我們要多次調用 <code class="docutils literal notranslate"><span class="pre">gradient</span></code> 方法進行求導，可以將 <code class="docutils literal notranslate"><span class="pre">persistent=True</span></code> 參數傳入 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> ，使得該記錄器持久保持記錄的信息。並在求導完成後手工使用 <code class="docutils literal notranslate"><span class="pre">del</span></code> 釋放記錄器資源。以下示例展示了用一個持久的記錄器 <code class="docutils literal notranslate"><span class="pre">tape</span></code> 分別計算 <img class="math" src="../../_images/math/2261dfa63cebc11353c25f3b42ab8812925703e7.png" alt="y(x) = x^2"/> 在 <img class="math" src="../../_images/math/6242bf7ca264a34d82d13f3307438c9bd77b8e15.png" alt="x = 3"/> 時的導數，以及 <img class="math" src="../../_images/math/7893faf5cbb0aecc221cadd4dbe71a05e3233c52.png" alt="y(x) = x^3"/> 在 <img class="math" src="../../_images/math/bf37d4838174ef61fbf93a49bc09c17de8674300.png" alt="x = 2"/> 時的導數。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">x_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">3.</span><span class="p">)</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">y_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span>
    <span class="n">y_2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x_2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y_grad_1</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y_1</span><span class="p">,</span> <span class="n">x_1</span><span class="p">)</span>  <span class="c1"># 6.0 = 2 * 3.0</span>
<span class="n">y_grad_2</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y_2</span><span class="p">,</span> <span class="n">x_2</span><span class="p">)</span>  <span class="c1"># 12.0 = 3 * 2.0 ^ 2</span>
<span class="k">del</span> <span class="n">tape</span>
</pre></div>
</div>
</div>
<div class="section" id="id5">
<h2>圖執行模式<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
<p>在圖執行模式（即使用 <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> 封裝計算圖）下也可以使用 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 。此時，其與 TensorFlow 1.X 中的 <code class="docutils literal notranslate"><span class="pre">tf.gradients</span></code> 基本等同。詳情見 <a class="reference internal" href="../../zh/advanced/static.html#graph-compare"><span class="std std-ref">自动求导机制的计算图对比 *</span></a> 。</p>
</div>
<div class="section" id="id6">
<h2>性能優化<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h2>
<p>由於 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 上下文中的任何計算步驟都會被記錄器所記錄，因此，爲了提高 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 的記錄效率，應當儘量只將需要求導的計算步驟封裝在 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 的上下文中。如果需要在中途臨時加入一些無需記錄求導的計算步驟，可以使用本章第一節介紹的 <code class="docutils literal notranslate"><span class="pre">tape.stop_recording()</span></code> 暫時停止上下文記錄器的記錄。同時，正如我們在本章「高階求導」一節所介紹的那樣，求導動作本身（即 <code class="docutils literal notranslate"><span class="pre">tape</span></code> 的 <code class="docutils literal notranslate"><span class="pre">gradient</span></code> 方法）也是一個計算步驟。因此，一般而言，除非需要進行高階求導，否則應當避免在 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 的上下文內調用其 <code class="docutils literal notranslate"><span class="pre">gradient</span></code> 方法。這會導致求導操作本身被 GradientTape 所記錄，從而造成效率的降低。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">3.</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y_grad</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>    <span class="c1"># 如果后续并不需要对 y_grad 求导，则不建议在上下文环境中求导</span>
    <span class="k">with</span> <span class="n">tape</span><span class="o">.</span><span class="n">stop_recording</span><span class="p">():</span>     <span class="c1"># 对于无需记录求导的计算步骤，可以暂停记录器后计算</span>
        <span class="n">y_grad_not_recorded</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">d2y_dx2</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y_grad</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># 如果后续需要对 y_grad 求导，则 y_grad 必须写在上下文中</span>
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h2>實例：對神經網絡的各層變量獨立求導<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h2>
<p>在實際的訓練流程中，我們有時需要對 <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code> 模型的部分變量求導，或者對模型不同部分的變量採取不同的優化策略。此時，我們可以通過模型中各個 <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code> 層的 <code class="docutils literal notranslate"><span class="pre">variables</span></code> 屬性取出層內的部分變量，並對這部分變量單獨應用優化器。以下示例展示了使用一個持久的 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 記錄器，對前節 <a class="reference internal" href="../basic/models.html#mlp"><span class="std std-ref">基礎範例：多層感知器（MLP）</span></a> 中多層感知機的第一層和第二層獨立進行優化的過程。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">zh.model.mnist.mlp</span> <span class="k">import</span> <span class="n">MLP</span>
<span class="kn">from</span> <span class="nn">zh.model.utils</span> <span class="k">import</span> <span class="n">MNISTLoader</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">learning_rate_1</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">learning_rate_2</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">()</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">MNISTLoader</span><span class="p">()</span>
<span class="c1"># 声明两个优化器，设定不同的学习率，分别用于更新MLP模型的第一层和第二层</span>
<span class="n">optimizer_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate_1</span><span class="p">)</span>
<span class="n">optimizer_2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate_2</span><span class="p">)</span>
<span class="n">num_batches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">data_loader</span><span class="o">.</span><span class="n">num_train_data</span> <span class="o">//</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_epochs</span><span class="p">)</span>
<span class="k">for</span> <span class="n">batch_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data_loader</span><span class="o">.</span><span class="n">get_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>  <span class="c1"># 声明一个持久的GradientTape，允许我们多次调用tape.gradient方法</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;batch </span><span class="si">%d</span><span class="s2">: loss </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">batch_index</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">dense1</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>    <span class="c1"># 单独求第一层参数的梯度</span>
    <span class="n">optimizer_1</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads_and_vars</span><span class="o">=</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">dense1</span><span class="o">.</span><span class="n">variables</span><span class="p">))</span> <span class="c1"># 单独对第一层参数更新，学习率0.001</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">dense2</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>    <span class="c1"># 单独求第二层参数的梯度</span>
    <span class="n">optimizer_2</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads_and_vars</span><span class="o">=</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">dense2</span><span class="o">.</span><span class="n">variables</span><span class="p">))</span> <span class="c1"># 单独对第二层参数更新，学习率0.01</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2021, Xihan Li (snowkylin)

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.

  <p><a href="https://beian.miit.gov.cn/" target="_blank">沪ICP备13038357号-18</a ></p> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-40509304-12', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>